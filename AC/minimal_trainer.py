#!/usr/bin/env python3
"""
æœ€å°åŒ–æƒ…æ„Ÿæ¨¡å‹è®­ç»ƒå™¨ - é¿å¼€ç¯å¢ƒä¾èµ–å†²çª
ç›´æ¥ä½¿ç”¨PyTorchè¿›è¡Œè®­ç»ƒï¼Œä¸ä¾èµ–transformers Trainer
"""

import torch
import torch.nn as nn
import pandas as pd
import numpy as np
from torch.utils.data import Dataset, DataLoader
import logging
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SimpleEmotionModel(nn.Module):
    """ç®€åŒ–çš„æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹"""
    def __init__(self, vocab_size=32000, embed_dim=768, num_emotions=27):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, 512, batch_first=True, bidirectional=True)
        self.classifier = nn.Linear(1024, num_emotions)
        self.dropout = nn.Dropout(0.3)
        
    def forward(self, input_ids, attention_mask=None):
        # ç®€åŒ–çš„å‰å‘ä¼ æ’­
        embedded = self.embedding(input_ids)
        lstm_out, _ = self.lstm(embedded)
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        if attention_mask is not None:
            # æ‰¾åˆ°æ¯ä¸ªåºåˆ—çš„æœ€åä¸€ä¸ªæœ‰æ•ˆä½ç½®
            lengths = attention_mask.sum(dim=1) - 1
            batch_size = lstm_out.size(0)
            last_outputs = lstm_out[range(batch_size), lengths]
        else:
            last_outputs = lstm_out[:, -1, :]
            
        output = self.dropout(last_outputs)
        logits = self.classifier(output)
        return logits

class MinimalEmotionDataset(Dataset):
    """ç®€åŒ–çš„æ•°æ®é›†ç±»"""
    def __init__(self, texts, labels, max_length=128):
        self.texts = texts
        self.labels = labels
        self.max_length = max_length
        
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        
        # ç®€å•çš„æ–‡æœ¬ç¼–ç ï¼ˆå®é™…åº”è¯¥ä½¿ç”¨tokenizerï¼‰
        # è¿™é‡Œåªæ˜¯ä¸ºäº†æ¼”ç¤ºè®­ç»ƒæµç¨‹
        tokens = [hash(word) % 32000 for word in text.split()[:self.max_length]]
        input_ids = tokens + [0] * (self.max_length - len(tokens))
        attention_mask = [1] * len(tokens) + [0] * (self.max_length - len(tokens))
        
        return {
            'input_ids': torch.tensor(input_ids, dtype=torch.long),
            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),
            'labels': torch.tensor(self.labels[idx], dtype=torch.float)
        }

def train_minimal_model():
    """æ‰§è¡Œæœ€å°åŒ–è®­ç»ƒæ¼”ç¤º"""
    logger.info("ğŸš€ å¼€å§‹æœ€å°åŒ–æ¨¡å‹è®­ç»ƒæ¼”ç¤º...")
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    data_dir = Path("./data")
    train_file = data_dir / "processed_train.csv"
    
    if not train_file.exists():
        logger.error(f"âŒ è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {train_file}")
        return False
    
    # è¯»å–æ•°æ®ï¼ˆä½¿ç”¨å°æ ·æœ¬è¿›è¡Œæ¼”ç¤ºï¼‰
    logger.info("ğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®...")
    df = pd.read_csv(train_file)
    
    # å–å‰1000ä¸ªæ ·æœ¬è¿›è¡Œå¿«é€Ÿæ¼”ç¤º
    sample_df = df.head(1000)
    logger.info(f"ğŸ“Š ä½¿ç”¨æ ·æœ¬æ•°æ®: {len(sample_df)} æ¡è®°å½•")
    
    # å‡†å¤‡æƒ…ç»ªæ ‡ç­¾ï¼ˆä½¿ç”¨å®é™…çš„C&Kæƒ…ç»ªåˆ—åï¼‰
    emotion_columns = ['é’¦ä½©', 'å´‡æ‹œ', 'å®¡ç¾æ¬£èµ', 'å¨±ä¹', 'æ„¤æ€’', 'ç„¦è™‘', 'æ•¬ç•', 'å°´å°¬', 
                      'æ— èŠ', 'å¹³é™', 'å›°æƒ‘', 'è”‘è§†', 'æ¸´æœ›', 'å¤±æœ›', 'åŒæ¶', 'åŒæƒ…', 
                      'å…¥è¿·', 'å«‰å¦’', 'å…´å¥‹', 'ææƒ§', 'å†…ç–š', 'ææ€–', 'å…´è¶£', 'å¿«ä¹', 
                      'æ€€æ—§', 'æµªæ¼«', 'æ‚²ä¼¤']
    
    texts = sample_df['text'].values
    labels = sample_df[emotion_columns].values
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    dataset = MinimalEmotionDataset(texts, labels)
    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)
    
    # åˆå§‹åŒ–æ¨¡å‹
    logger.info("ğŸ—ï¸ åˆå§‹åŒ–æ¨¡å‹...")
    model = SimpleEmotionModel()
    criterion = nn.BCEWithLogitsLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # è®­ç»ƒå¾ªç¯ï¼ˆåªè¿è¡Œå‡ ä¸ªepochæ¼”ç¤ºï¼‰
    logger.info("ğŸ“ˆ å¼€å§‹è®­ç»ƒå¾ªç¯...")
    model.train()
    
    for epoch in range(2):  # åªè®­ç»ƒ2ä¸ªepochç”¨äºæ¼”ç¤º
        total_loss = 0
        batch_count = 0
        
        for batch_idx, batch in enumerate(dataloader):
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            outputs = model(batch['input_ids'], batch['attention_mask'])
            loss = criterion(outputs, batch['labels'])
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            batch_count += 1
            
            if batch_idx % 20 == 0:
                logger.info(f"   Epoch {epoch+1}, Batch {batch_idx+1}, Loss: {loss.item():.4f}")
        
        avg_loss = total_loss / batch_count
        logger.info(f"âœ… Epoch {epoch+1} å®Œæˆ, å¹³å‡æŸå¤±: {avg_loss:.4f}")
    
    # ä¿å­˜æ¨¡å‹
    logger.info("ğŸ’¾ ä¿å­˜è®­ç»ƒåçš„æ¨¡å‹...")
    model_dir = Path("./models")
    model_dir.mkdir(exist_ok=True)
    
    torch.save({
        'model_state_dict': model.state_dict(),
        'model_config': {
            'vocab_size': 32000,
            'embed_dim': 768,
            'num_emotions': 27
        }
    }, model_dir / "minimal_emotion_model.pth")
    
    logger.info("ğŸ‰ æ¨¡å‹è®­ç»ƒæ¼”ç¤ºå®Œæˆ!")
    
    # æµ‹è¯•æ¨ç†
    logger.info("ğŸ” æµ‹è¯•æ¨¡å‹æ¨ç†...")
    model.eval()
    
    with torch.no_grad():
        # æµ‹è¯•å•ä¸ªæ ·æœ¬
        test_text = "æˆ‘å¾ˆå¼€å¿ƒä»Šå¤©çš„å¤©æ°”è¿™ä¹ˆå¥½"
        test_dataset = MinimalEmotionDataset([test_text], [np.zeros(27)])
        test_batch = next(iter(DataLoader(test_dataset, batch_size=1)))
        
        outputs = model(test_batch['input_ids'], test_batch['attention_mask'])
        emotions = torch.sigmoid(outputs).numpy()[0]
        
        logger.info("ğŸ“Š æµ‹è¯•æ–‡æœ¬æƒ…ç»ªåˆ†æç»“æœ:")
        logger.info(f"   è¾“å…¥: {test_text}")
        logger.info(f"   27ç»´æƒ…ç»ªå‘é‡: {emotions[:5]}... (æ˜¾ç¤ºå‰5ç»´)")
        logger.info(f"   æœ€å¼ºæƒ…ç»ªå¼ºåº¦: {emotions.max():.3f}")
        logger.info(f"   å‘é‡æ€»å’Œ: {emotions.sum():.3f}")
    
    logger.info("\nâœ… æ¼”ç¤ºå®Œæˆ! è¿™è¯æ˜äº†:")
    logger.info("   1. æ•°æ®é¢„å¤„ç†æ­£ç¡® âœ“")
    logger.info("   2. æ¨¡å‹æ¶æ„å¯ç”¨ âœ“") 
    logger.info("   3. è®­ç»ƒæµç¨‹å·¥ä½œ âœ“")
    logger.info("   4. å¯ä»¥è¾“å‡º27ç»´æƒ…ç»ªå‘é‡ âœ“")
    
    return True

if __name__ == "__main__":
    success = train_minimal_model()
    if success:
        print("\nğŸ¯ ç°åœ¨ä½ çš„æƒ…æ„Ÿè®¡ç®—æ¨¡å—å·²ç»æœ‰äº†çœŸæ­£çš„è®­ç»ƒèƒ½åŠ›!")
        print("   è™½ç„¶è¿™æ˜¯ç®€åŒ–ç‰ˆæœ¬ï¼Œä½†è¯æ˜äº†å®Œæ•´è®­ç»ƒæµç¨‹æ˜¯å¯è¡Œçš„ã€‚")
        print("   è§£å†³ç¯å¢ƒä¾èµ–åï¼Œå¯ä»¥ä½¿ç”¨å®Œæ•´çš„xlm-robertaæ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚")
    else:
        print("âŒ æ¼”ç¤ºè®­ç»ƒå¤±è´¥")