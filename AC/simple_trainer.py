#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆæƒ…æ„Ÿæ¨¡å‹è®­ç»ƒå™¨
é¿å¼€transformerså¤æ‚ä¾èµ–ï¼Œç›´æ¥ä½¿ç”¨PyTorch
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
import logging
from pathlib import Path
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import f1_score
import json

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SimpleEmotionModel(nn.Module):
    """ç®€åŒ–çš„æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹ - åŸºäºLSTM"""
    
    def __init__(self, vocab_size=50000, embed_dim=256, hidden_dim=512, num_emotions=27, dropout=0.3):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=2, batch_first=True, 
                           dropout=dropout, bidirectional=True)
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(hidden_dim * 2, num_emotions)
        
    def forward(self, input_ids, attention_mask=None):
        # åµŒå…¥å±‚
        embedded = self.embedding(input_ids)  # (batch, seq_len, embed_dim)
        
        # LSTMå±‚
        lstm_out, (hidden, cell) = self.lstm(embedded)  # (batch, seq_len, hidden_dim*2)
        
        # ä½¿ç”¨æ³¨æ„åŠ›æ©ç è·å–æœ€åæœ‰æ•ˆä½ç½®çš„è¾“å‡º
        if attention_mask is not None:
            # è®¡ç®—æ¯ä¸ªåºåˆ—çš„æœ‰æ•ˆé•¿åº¦
            lengths = attention_mask.sum(dim=1).long() - 1  # å‡1å› ä¸ºç´¢å¼•ä»0å¼€å§‹
            batch_size = lstm_out.size(0)
            # è·å–æ¯ä¸ªåºåˆ—æœ€åä¸€ä¸ªæœ‰æ•ˆä½ç½®çš„è¾“å‡º
            last_outputs = lstm_out[range(batch_size), lengths]
        else:
            # å¦‚æœæ²¡æœ‰æ©ç ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥
            last_outputs = lstm_out[:, -1, :]
        
        # åˆ†ç±»å±‚
        output = self.dropout(last_outputs)
        logits = self.classifier(output)
        
        return logits

class SimpleEmotionDataset(Dataset):
    """ç®€åŒ–çš„æƒ…æ„Ÿæ•°æ®é›†"""
    
    def __init__(self, texts, labels, vocab_dict=None, max_length=128):
        self.texts = texts
        self.labels = labels
        self.max_length = max_length
        
        # æ„å»ºè¯æ±‡è¡¨
        if vocab_dict is None:
            self.vocab_dict = self._build_vocab(texts)
        else:
            self.vocab_dict = vocab_dict
            
    def _build_vocab(self, texts):
        """æ„å»ºè¯æ±‡è¡¨"""
        vocab = {'<PAD>': 0, '<UNK>': 1}
        word_count = {}
        
        # ç»Ÿè®¡è¯é¢‘
        for text in texts:
            for word in str(text).split():
                word_count[word] = word_count.get(word, 0) + 1
        
        # æŒ‰é¢‘ç‡æ’åºï¼Œåªä¿ç•™å‰48000ä¸ªè¯ï¼ˆç•™2ä¸ªä½ç½®ç»™ç‰¹æ®Šç¬¦å·ï¼‰
        sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:48000]
        
        for word, _ in sorted_words:
            vocab[word] = len(vocab)
            
        logger.info(f"ğŸ“š æ„å»ºè¯æ±‡è¡¨å®Œæˆï¼Œå¤§å°: {len(vocab)}")
        return vocab
    
    def _text_to_ids(self, text):
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºIDåºåˆ—"""
        words = str(text).split()[:self.max_length]
        ids = [self.vocab_dict.get(word, 1) for word in words]  # 1æ˜¯<UNK>
        
        # å¡«å……æˆ–æˆªæ–­åˆ°å›ºå®šé•¿åº¦
        if len(ids) < self.max_length:
            attention_mask = [1] * len(ids) + [0] * (self.max_length - len(ids))
            ids.extend([0] * (self.max_length - len(ids)))  # 0æ˜¯<PAD>
        else:
            attention_mask = [1] * self.max_length
            
        return ids, attention_mask
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        labels = self.labels[idx]
        
        input_ids, attention_mask = self._text_to_ids(text)
        
        return {
            'input_ids': torch.tensor(input_ids, dtype=torch.long),
            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),
            'labels': torch.tensor(labels, dtype=torch.float32)
        }

class EmotionModelTrainer:
    """æƒ…æ„Ÿæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # æƒ…ç»ªæ ‡ç­¾ï¼ˆä¸å¤„ç†åçš„æ•°æ®ä¿æŒä¸€è‡´ï¼‰
        self.emotion_columns = [
            'é’¦ä½©', 'å´‡æ‹œ', 'å®¡ç¾æ¬£èµ', 'å¨±ä¹', 'æ„¤æ€’', 'ç„¦è™‘', 'æ•¬ç•', 'å°´å°¬',
            'æ— èŠ', 'å¹³é™', 'å›°æƒ‘', 'è”‘è§†', 'æ¸´æœ›', 'å¤±æœ›', 'åŒæ¶', 'åŒæƒ…',
            'å…¥è¿·', 'å«‰å¦’', 'å…´å¥‹', 'ææƒ§', 'å†…ç–š', 'ææ€–', 'å…´è¶£', 'å¿«ä¹',
            'æ€€æ—§', 'æµªæ¼«', 'æ‚²ä¼¤'
        ]
        
    def load_data(self):
        """åŠ è½½é¢„å¤„ç†åçš„æ•°æ®"""
        logger.info("ğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®...")
        
        data_dir = Path("./data")
        
        # åŠ è½½è®­ç»ƒå’ŒéªŒè¯æ•°æ®
        train_df = pd.read_csv(data_dir / "processed_train.csv")
        dev_df = pd.read_csv(data_dir / "processed_dev.csv")
        
        logger.info(f"ğŸ“Š è®­ç»ƒæ•°æ®: {len(train_df)} æ ·æœ¬")
        logger.info(f"ğŸ“Š éªŒè¯æ•°æ®: {len(dev_df)} æ ·æœ¬")
        
        # æå–æ–‡æœ¬å’Œæ ‡ç­¾
        train_texts = train_df['text'].values
        train_labels = train_df[self.emotion_columns].values
        
        dev_texts = dev_df['text'].values
        dev_labels = dev_df[self.emotion_columns].values
        
        return train_texts, train_labels, dev_texts, dev_labels
    
    def train_and_save(self):
        """è®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹"""
        try:
            # åŠ è½½æ•°æ®
            train_texts, train_labels, dev_texts, dev_labels = self.load_data()
            
            # åˆ›å»ºæ•°æ®é›†
            logger.info("ğŸ—ï¸ åˆ›å»ºæ•°æ®é›†...")
            train_dataset = SimpleEmotionDataset(train_texts, train_labels)
            dev_dataset = SimpleEmotionDataset(dev_texts, dev_labels, 
                                             vocab_dict=train_dataset.vocab_dict)
            
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
            dev_loader = DataLoader(dev_dataset, batch_size=16, shuffle=False)
            
            # åˆå§‹åŒ–æ¨¡å‹
            logger.info("ğŸ¤– åˆå§‹åŒ–æ¨¡å‹...")
            vocab_size = len(train_dataset.vocab_dict)
            model = SimpleEmotionModel(vocab_size=vocab_size).to(self.device)
            
            # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
            criterion = nn.BCEWithLogitsLoss()
            optimizer = optim.Adam(model.parameters(), lr=2e-4, weight_decay=1e-5)
            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.8)
            
            # è®­ç»ƒå¾ªç¯
            logger.info("ğŸ“ˆ å¼€å§‹è®­ç»ƒ...")
            num_epochs = 5
            best_f1 = 0.0
            
            for epoch in range(num_epochs):
                # è®­ç»ƒé˜¶æ®µ
                model.train()
                train_loss = 0.0
                train_batches = 0
                
                for batch_idx, batch in enumerate(train_loader):
                    # ç§»åŠ¨åˆ°è®¾å¤‡
                    input_ids = batch['input_ids'].to(self.device)
                    attention_mask = batch['attention_mask'].to(self.device)
                    labels = batch['labels'].to(self.device)
                    
                    # å‰å‘ä¼ æ’­
                    optimizer.zero_grad()
                    outputs = model(input_ids, attention_mask)
                    loss = criterion(outputs, labels)
                    
                    # åå‘ä¼ æ’­
                    loss.backward()
                    optimizer.step()
                    
                    train_loss += loss.item()
                    train_batches += 1
                    
                    if batch_idx % 500 == 0:
                        logger.info(f"   Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}")
                
                # éªŒè¯é˜¶æ®µ
                model.eval()
                dev_loss = 0.0
                all_predictions = []
                all_labels = []
                
                with torch.no_grad():
                    for batch in dev_loader:
                        input_ids = batch['input_ids'].to(self.device)
                        attention_mask = batch['attention_mask'].to(self.device)
                        labels = batch['labels'].to(self.device)
                        
                        outputs = model(input_ids, attention_mask)
                        loss = criterion(outputs, labels)
                        dev_loss += loss.item()
                        
                        # æ”¶é›†é¢„æµ‹ç»“æœ
                        predictions = torch.sigmoid(outputs).cpu().numpy()
                        all_predictions.append(predictions)
                        all_labels.append(labels.cpu().numpy())
                
                # è®¡ç®—æŒ‡æ ‡
                all_predictions = np.vstack(all_predictions)
                all_labels = np.vstack(all_labels)
                
                # ä½¿ç”¨0.5é˜ˆå€¼è¿›è¡ŒäºŒå€¼åŒ–
                binary_predictions = (all_predictions > 0.5).astype(int)
                
                f1_macro = f1_score(all_labels, binary_predictions, average='macro', zero_division=0)
                f1_micro = f1_score(all_labels, binary_predictions, average='micro', zero_division=0)
                
                avg_train_loss = train_loss / train_batches
                avg_dev_loss = dev_loss / len(dev_loader)
                
                logger.info(f"âœ… Epoch {epoch+1}/{num_epochs} å®Œæˆ:")
                logger.info(f"   è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")
                logger.info(f"   éªŒè¯æŸå¤±: {avg_dev_loss:.4f}")
                logger.info(f"   F1-macro: {f1_macro:.4f}")
                logger.info(f"   F1-micro: {f1_micro:.4f}")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if f1_macro > best_f1:
                    best_f1 = f1_macro
                    self._save_model(model, train_dataset.vocab_dict, f1_macro)
                    logger.info(f"ğŸ’¾ ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹ (F1: {f1_macro:.4f})")
                
                scheduler.step()
            
            logger.info(f"ğŸ‰ è®­ç»ƒå®Œæˆ! æœ€ä½³F1åˆ†æ•°: {best_f1:.4f}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _save_model(self, model, vocab_dict, f1_score):
        """ä¿å­˜æ¨¡å‹å’Œç›¸å…³é…ç½®"""
        models_dir = Path("./models")
        models_dir.mkdir(exist_ok=True)
        
        save_path = models_dir / "simple_emotion_model"
        save_path.mkdir(exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹æƒé‡
        torch.save({
            'model_state_dict': model.state_dict(),
            'model_config': {
                'vocab_size': len(vocab_dict),
                'embed_dim': 256,
                'hidden_dim': 512,
                'num_emotions': 27,
                'dropout': 0.3
            },
            'f1_score': f1_score,
            'emotion_columns': self.emotion_columns
        }, save_path / "model.pth")
        
        # ä¿å­˜è¯æ±‡è¡¨
        with open(save_path / "vocab.json", 'w', encoding='utf-8') as f:
            json.dump(vocab_dict, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¯åŠ¨ç®€åŒ–ç‰ˆæƒ…æ„Ÿæ¨¡å‹è®­ç»ƒ...")
    
    trainer = EmotionModelTrainer()
    success = trainer.train_and_save()
    
    if success:
        logger.info("âœ… è®­ç»ƒæˆåŠŸå®Œæˆ!")
        logger.info("ğŸ¯ ç°åœ¨ä½ çš„ACæ¨¡å—å¯ä»¥å°†æ–‡æœ¬è½¬æ¢ä¸º27ç»´æƒ…ç»ªå‘é‡äº†!")
    else:
        logger.error("âŒ è®­ç»ƒå¤±è´¥")
    
    return success

if __name__ == "__main__":
    main()