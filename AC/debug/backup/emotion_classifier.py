#!/usr/bin/env python3
"""
æƒ…æ„Ÿåˆ†ç±»å™¨æ ¸å¿ƒæ¨¡å—

åŸºäºxlm-robertaçš„å¤šè¯­è¨€æƒ…æ„Ÿåˆ†ç±»å™¨
æ”¯æŒæ–‡æœ¬è¾“å…¥åˆ°27ç»´C&Kæƒ…ç»ªå‘é‡çš„ç«¯åˆ°ç«¯è½¬æ¢
"""

import torch
import torch.nn as nn
import numpy as np
import logging
from typing import Dict, List, Union, Tuple, Optional
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    AutoConfig,
    Trainer,
    TrainingArguments,
    EvalPrediction
)
from sklearn.metrics import accuracy_score, f1_score
import warnings
warnings.filterwarnings("ignore")

try:
    from .config import MODEL_CONFIG, MODEL_PATHS, COWEN_KELTNER_EMOTIONS, INFERENCE_CONFIG
    from .emotion_mapper import GoEmotionsMapper
except ImportError:
    from config import MODEL_CONFIG, MODEL_PATHS, COWEN_KELTNER_EMOTIONS, INFERENCE_CONFIG
    from emotion_mapper import GoEmotionsMapper

logger = logging.getLogger(__name__)

class EmotionClassifier(nn.Module):
    """åŸºäºxlm-robertaçš„æƒ…æ„Ÿåˆ†ç±»å™¨"""
    
    def __init__(self, model_name: str = None, num_labels: int = 27, load_pretrained: bool = True):
        """
        åˆå§‹åŒ–æƒ…æ„Ÿåˆ†ç±»å™¨
        
        Args:
            model_name: é¢„è®­ç»ƒæ¨¡å‹åç§°
            num_labels: è¾“å‡ºæ ‡ç­¾æ•°é‡ (27ç»´C&Kæƒ…ç»ª)
            load_pretrained: æ˜¯å¦åŠ è½½é¢„è®­ç»ƒæƒé‡
        """
        super().__init__()
        
        self.model_name = model_name or MODEL_CONFIG["model_name"]
        self.num_labels = num_labels
        self.emotion_names = COWEN_KELTNER_EMOTIONS
        
        # è®¾å¤‡æ£€æµ‹
        self.device = self._detect_device()
        logger.info(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–åˆ†è¯å™¨å’Œæ¨¡å‹
        if load_pretrained:
            self._load_pretrained_model()
        
        # åˆå§‹åŒ–æ˜ å°„å™¨
        self.mapper = GoEmotionsMapper()
        
        logger.info("âœ… æƒ…æ„Ÿåˆ†ç±»å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _detect_device(self) -> str:
        """æ£€æµ‹å¯ç”¨è®¾å¤‡"""
        if torch.cuda.is_available():
            return "cuda"
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            return "mps"
        else:
            return "cpu"
    
    def _load_pretrained_model(self):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        try:
            logger.info(f"ğŸ“¥ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {self.model_name}")
            
            # é…ç½®
            config = AutoConfig.from_pretrained(
                self.model_name,
                num_labels=self.num_labels,
                problem_type="multi_label_classification",
                cache_dir=MODEL_PATHS["pretrained_cache"]
            )
            
            # åˆ†è¯å™¨
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                cache_dir=MODEL_PATHS["pretrained_cache"]
            )
            
            # æ¨¡å‹
            self.model = AutoModelForSequenceClassification.from_pretrained(
                self.model_name,
                config=config,
                cache_dir=MODEL_PATHS["pretrained_cache"]
            )
            
            # ç§»åˆ°è®¾å¤‡
            self.model.to(self.device)
            
            logger.info(f"âœ… é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ é¢„è®­ç»ƒæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def load_finetuned_model(self, model_path: str = None):
        """
        åŠ è½½å¾®è°ƒåçš„æ¨¡å‹
        
        Args:
            model_path: å¾®è°ƒæ¨¡å‹è·¯å¾„
        """
        try:
            model_path = model_path or MODEL_PATHS["finetuned_model"]
            logger.info(f"ğŸ“¥ åŠ è½½å¾®è°ƒæ¨¡å‹: {model_path}")
            
            # åŠ è½½åˆ†è¯å™¨
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            
            # åŠ è½½æ¨¡å‹
            self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
            self.model.to(self.device)
            self.model.eval()
            
            logger.info(f"âœ… å¾®è°ƒæ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ å¾®è°ƒæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # å›é€€åˆ°é¢„è®­ç»ƒæ¨¡å‹
            logger.info("ğŸ”„ å›é€€åˆ°é¢„è®­ç»ƒæ¨¡å‹")
            self._load_pretrained_model()
    
    def predict_single(self, text: str, return_dict: bool = False) -> Union[np.ndarray, Dict[str, float]]:
        """
        å•æ–‡æœ¬æƒ…æ„Ÿé¢„æµ‹
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            return_dict: æ˜¯å¦è¿”å›å­—å…¸æ ¼å¼
            
        Returns:
            27ç»´æƒ…ç»ªå‘é‡æˆ–æƒ…ç»ªå­—å…¸
        """
        try:
            if not text or len(text.strip()) < 1:
                logger.warning("âš ï¸  è¾“å…¥æ–‡æœ¬ä¸ºç©ºï¼Œè¿”å›é›¶å‘é‡")
                zero_vector = np.zeros(27, dtype=np.float32)
                return self.mapper.map_ck_vector_to_dict(zero_vector) if return_dict else zero_vector
            
            # æ–‡æœ¬é¢„å¤„ç†å’Œåˆ†è¯
            inputs = self.tokenizer(
                text,
                padding=True,
                truncation=True,
                max_length=MODEL_CONFIG["max_length"],
                return_tensors="pt"
            )
            
            # ç§»åˆ°è®¾å¤‡
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # æ¨¡å‹æ¨ç†
            with torch.no_grad():
                outputs = self.model(**inputs)
                logits = outputs.logits
                
                # åº”ç”¨sigmoidæ¿€æ´» (å¤šæ ‡ç­¾åˆ†ç±»)
                probabilities = torch.sigmoid(logits).cpu().numpy().flatten()
            
            # ç¡®ä¿è¾“å‡ºç»´åº¦æ­£ç¡®
            if len(probabilities) != 27:
                logger.error(f"âŒ æ¨¡å‹è¾“å‡ºç»´åº¦é”™è¯¯: æœŸæœ›27ç»´ï¼Œå®é™…{len(probabilities)}ç»´")
                probabilities = np.zeros(27, dtype=np.float32)
            
            # åº”ç”¨ç½®ä¿¡åº¦é˜ˆå€¼
            threshold = INFERENCE_CONFIG["confidence_threshold"]
            probabilities = np.where(probabilities > threshold, probabilities, 0.0)
            
            # å½’ä¸€åŒ–åˆ°[0, 1]
            probabilities = np.clip(probabilities, 0, 1)
            
            if return_dict:
                return self.mapper.map_ck_vector_to_dict(probabilities)
            else:
                return probabilities.astype(np.float32)
                
        except Exception as e:
            logger.error(f"âŒ å•æ–‡æœ¬é¢„æµ‹å¤±è´¥: {e}")
            zero_vector = np.zeros(27, dtype=np.float32)
            return self.mapper.map_ck_vector_to_dict(zero_vector) if return_dict else zero_vector
    
    def predict_batch(self, texts: List[str], batch_size: int = None) -> np.ndarray:
        """
        æ‰¹é‡æ–‡æœ¬æƒ…æ„Ÿé¢„æµ‹
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            (N, 27) æƒ…ç»ªå‘é‡çŸ©é˜µ
        """
        try:
            if not texts:
                return np.zeros((0, 27), dtype=np.float32)
            
            batch_size = batch_size or INFERENCE_CONFIG["max_batch_size"]
            results = []
            
            logger.info(f"ğŸ”„ å¼€å§‹æ‰¹é‡é¢„æµ‹: {len(texts)} æ¡æ–‡æœ¬")
            
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                
                # æ‰¹é‡åˆ†è¯
                inputs = self.tokenizer(
                    batch_texts,
                    padding=True,
                    truncation=True,
                    max_length=MODEL_CONFIG["max_length"],
                    return_tensors="pt"
                )
                
                # ç§»åˆ°è®¾å¤‡
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                # æ‰¹é‡æ¨ç†
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    logits = outputs.logits
                    
                    # åº”ç”¨sigmoidæ¿€æ´»
                    probabilities = torch.sigmoid(logits).cpu().numpy()
                
                results.append(probabilities)
                
                if (i + batch_size) % 100 == 0:
                    logger.info(f"   æ‰¹é‡é¢„æµ‹è¿›åº¦: {min(i + batch_size, len(texts))}/{len(texts)}")
            
            # åˆå¹¶ç»“æœ
            all_results = np.vstack(results)
            
            # åº”ç”¨é˜ˆå€¼å’Œå½’ä¸€åŒ–
            threshold = INFERENCE_CONFIG["confidence_threshold"]
            all_results = np.where(all_results > threshold, all_results, 0.0)
            all_results = np.clip(all_results, 0, 1)
            
            logger.info(f"âœ… æ‰¹é‡é¢„æµ‹å®Œæˆ: {all_results.shape}")
            return all_results.astype(np.float32)
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡é¢„æµ‹å¤±è´¥: {e}")
            return np.zeros((len(texts), 27), dtype=np.float32)
    
    def get_top_emotions(self, text: str, top_k: int = 5) -> List[Tuple[str, float]]:
        """
        è·å–æ–‡æœ¬çš„top-kæƒ…ç»ª
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            top_k: è¿”å›å‰kä¸ªæƒ…ç»ª
            
        Returns:
            [(æƒ…ç»ªå, å¼ºåº¦å€¼), ...]
        """
        emotion_vector = self.predict_single(text)
        return self.mapper.get_top_emotions_from_vector(emotion_vector, top_k)
    
    def analyze_emotion_distribution(self, texts: List[str]) -> Dict[str, any]:
        """
        åˆ†ææ–‡æœ¬é›†åˆçš„æƒ…ç»ªåˆ†å¸ƒ
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            æƒ…ç»ªåˆ†å¸ƒç»Ÿè®¡
        """
        try:
            # æ‰¹é‡é¢„æµ‹
            emotion_matrix = self.predict_batch(texts)
            
            # ç»Ÿè®¡åˆ†æ
            stats = {}
            
            # æ¯ä¸ªæƒ…ç»ªçš„å¹³å‡å¼ºåº¦
            mean_intensities = np.mean(emotion_matrix, axis=0)
            stats["mean_emotions"] = {
                emotion: float(mean_intensities[i]) 
                for i, emotion in enumerate(self.emotion_names)
            }
            
            # æœ€å¸¸è§çš„ä¸»å¯¼æƒ…ç»ª
            dominant_emotions = [self.emotion_names[np.argmax(row)] for row in emotion_matrix]
            from collections import Counter
            stats["dominant_distribution"] = dict(Counter(dominant_emotions))
            
            # æƒ…ç»ªæ´»è·ƒåº¦ (éé›¶æƒ…ç»ªçš„å¹³å‡æ•°é‡)
            active_emotions = np.sum(emotion_matrix > 0, axis=1)
            stats["avg_active_emotions"] = float(np.mean(active_emotions))
            
            # æ€»ä½“æƒ…ç»ªå¼ºåº¦
            total_intensities = np.sum(emotion_matrix, axis=1)
            stats["avg_total_intensity"] = float(np.mean(total_intensities))
            
            return stats
            
        except Exception as e:
            logger.error(f"âŒ æƒ…ç»ªåˆ†å¸ƒåˆ†æå¤±è´¥: {e}")
            return {}
    
    def save_model(self, output_path: str = None):
        """
        ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹
        
        Args:
            output_path: è¾“å‡ºè·¯å¾„
        """
        try:
            output_path = output_path or MODEL_PATHS["finetuned_model"]
            logger.info(f"ğŸ’¾ ä¿å­˜æ¨¡å‹åˆ°: {output_path}")
            
            # ä¿å­˜æ¨¡å‹å’Œåˆ†è¯å™¨
            self.model.save_pretrained(output_path)
            self.tokenizer.save_pretrained(output_path)
            
            logger.info("âœ… æ¨¡å‹ä¿å­˜æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
            raise

def main():
    """æµ‹è¯•æƒ…æ„Ÿåˆ†ç±»å™¨"""
    print("ğŸ§  æƒ…æ„Ÿåˆ†ç±»å™¨æµ‹è¯•")
    print("=" * 50)
    
    # åˆå§‹åŒ–åˆ†ç±»å™¨
    classifier = EmotionClassifier(load_pretrained=True)
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "æˆ‘ä»Šå¤©æ„Ÿåˆ°éå¸¸å¼€å¿ƒå’Œå…´å¥‹ï¼",
        "I am feeling very anxious about the exam tomorrow.",
        "è¿™é¦–éŸ³ä¹è®©æˆ‘æ„Ÿåˆ°å¹³é™å’Œæ”¾æ¾ã€‚",
        "Je suis trÃ¨s en colÃ¨re contre cette situation!",
        "The sunset is absolutely beautiful and awe-inspiring."
    ]
    
    print(f"\nğŸ§ª æµ‹è¯•å•æ–‡æœ¬é¢„æµ‹:")
    for text in test_texts:
        # é¢„æµ‹æƒ…ç»ªå‘é‡
        emotion_vector = classifier.predict_single(text)
        
        # è·å–top-3æƒ…ç»ª
        top_emotions = classifier.get_top_emotions(text, 3)
        
        print(f"\næ–‡æœ¬: {text}")
        print(f"ä¸»è¦æƒ…ç»ª: {top_emotions}")
        print(f"å‘é‡å¼ºåº¦: {np.sum(emotion_vector):.3f}")
    
    # æµ‹è¯•æ‰¹é‡é¢„æµ‹
    print(f"\nğŸ”„ æµ‹è¯•æ‰¹é‡é¢„æµ‹:")
    batch_results = classifier.predict_batch(test_texts)
    print(f"æ‰¹é‡ç»“æœå½¢çŠ¶: {batch_results.shape}")
    
    # åˆ†ææƒ…ç»ªåˆ†å¸ƒ
    print(f"\nğŸ“Š æƒ…ç»ªåˆ†å¸ƒåˆ†æ:")
    distribution = classifier.analyze_emotion_distribution(test_texts)
    print(f"å¹³å‡æ´»è·ƒæƒ…ç»ªæ•°: {distribution.get('avg_active_emotions', 0):.2f}")
    print(f"ä¸»å¯¼æƒ…ç»ªåˆ†å¸ƒ: {distribution.get('dominant_distribution', {})}")
    
    print(f"\nâœ… æƒ…æ„Ÿåˆ†ç±»å™¨æµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    main()