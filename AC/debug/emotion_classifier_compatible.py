#!/usr/bin/env python3
"""
ç‰ˆæœ¬å…¼å®¹çš„æƒ…æ„Ÿåˆ†ç±»å™¨ - ä¿®å¤ç‰ˆ

ä¸»è¦ä¿®å¤:
1. åˆ†è¯å™¨åŠ è½½å…¼å®¹æ€§
2. æ¨¡å‹ç»“æ„è®¿é—®é€‚é…
3. è®¾å¤‡æ£€æµ‹ä¼˜åŒ–
4. é”™è¯¯å¤„ç†å¢å¼º
"""

import torch
import torch.nn as nn
import numpy as np
import logging
from typing import Dict, List, Union, Tuple, Optional
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    AutoConfig
)

try:
    from .config import MODEL_CONFIG, MODEL_PATHS, COWEN_KELTNER_EMOTIONS, INFERENCE_CONFIG
    from .emotion_mapper import GoEmotionsMapper
except ImportError:
    from config import MODEL_CONFIG, MODEL_PATHS, COWEN_KELTNER_EMOTIONS, INFERENCE_CONFIG
    from emotion_mapper import GoEmotionsMapper

logger = logging.getLogger(__name__)

class CompatibleEmotionClassifier(nn.Module):
    """ç‰ˆæœ¬å…¼å®¹çš„æƒ…æ„Ÿåˆ†ç±»å™¨"""
    
    def __init__(self, model_name: str = None, num_labels: int = 27, load_pretrained: bool = True):
        """åˆå§‹åŒ–å…¼å®¹ç‰ˆåˆ†ç±»å™¨"""
        super().__init__()
        
        self.model_name = model_name or MODEL_CONFIG["model_name"]
        self.num_labels = num_labels
        self.emotion_names = COWEN_KELTNER_EMOTIONS
        
        # è®¾å¤‡æ£€æµ‹ - å¢å¼ºå…¼å®¹æ€§
        self.device = self._detect_device()
        logger.info(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # æ¨¡å‹åŠ è½½æ ‡å¿—
        self.model_loaded = False
        self.tokenizer_loaded = False
        
        # åˆå§‹åŒ–æ¨¡å‹
        if load_pretrained:
            self._load_pretrained_model_safe()
        
        # åˆå§‹åŒ–æ˜ å°„å™¨
        self.mapper = GoEmotionsMapper()
        
        logger.info("âœ… å…¼å®¹ç‰ˆæƒ…æ„Ÿåˆ†ç±»å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _detect_device(self) -> str:
        """å¢å¼ºçš„è®¾å¤‡æ£€æµ‹"""
        if torch.cuda.is_available():
            return "cuda"
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            # æ£€æŸ¥MPSå®é™…å¯ç”¨æ€§
            try:
                test_tensor = torch.randn(1).to("mps")
                return "mps"
            except:
                return "cpu"
        else:
            return "cpu"
    
    def _load_pretrained_model_safe(self):
        """å®‰å…¨çš„é¢„è®­ç»ƒæ¨¡å‹åŠ è½½"""
        try:
            logger.info(f"ğŸ“¥ å°è¯•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {self.model_name}")
            
            # é¦–å…ˆå°è¯•åŠ è½½é…ç½®
            config = AutoConfig.from_pretrained(
                self.model_name,
                num_labels=self.num_labels,
                problem_type="multi_label_classification",
                cache_dir=MODEL_PATHS["pretrained_cache"]
            )
            
            # å°è¯•åŠ è½½åˆ†è¯å™¨ - å¤šç§ç­–ç•¥
            self.tokenizer = self._load_tokenizer_safe()
            
            # åŠ è½½æ¨¡å‹
            self.model = AutoModelForSequenceClassification.from_pretrained(
                self.model_name,
                config=config,
                cache_dir=MODEL_PATHS["pretrained_cache"]
            )
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            self.model.to(self.device)
            self.model_loaded = True
            
            logger.info("âœ… é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ é¢„è®­ç»ƒæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œä¿æŒä¼˜é›…é™çº§
    
    def _load_tokenizer_safe(self):
        """å®‰å…¨çš„åˆ†è¯å™¨åŠ è½½"""
        tokenizer = None
        
        # ç­–ç•¥1: ç›´æ¥ä»æ¨¡å‹è·¯å¾„åŠ è½½
        try:
            tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.tokenizer_loaded = True
            logger.info("âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ (ç­–ç•¥1)")
            return tokenizer
        except Exception as e:
            logger.warning(f"åˆ†è¯å™¨åŠ è½½ç­–ç•¥1å¤±è´¥: {e}")
        
        # ç­–ç•¥2: ä»åŸºç¡€æ¨¡å‹åŠ è½½
        try:
            tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base")
            self.tokenizer_loaded = True
            logger.info("âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ (ç­–ç•¥2: åŸºç¡€æ¨¡å‹)")
            return tokenizer
        except Exception as e:
            logger.error(f"æ‰€æœ‰åˆ†è¯å™¨åŠ è½½ç­–ç•¥å¤±è´¥: {e}")
            
        return tokenizer
    
    def load_finetuned_model_safe(self, model_path: str = None):
        """å®‰å…¨çš„å¾®è°ƒæ¨¡å‹åŠ è½½"""
        try:
            model_path = model_path or MODEL_PATHS["finetuned_model"]
            logger.info(f"ğŸ“¥ å°è¯•åŠ è½½å¾®è°ƒæ¨¡å‹: {model_path}")
            
            # æ£€æŸ¥è·¯å¾„å­˜åœ¨æ€§
            from pathlib import Path
            model_path = Path(model_path)
            if not model_path.exists():
                raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            
            # ç­–ç•¥1: ç›´æ¥åŠ è½½
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(str(model_path))
                self.model = AutoModelForSequenceClassification.from_pretrained(str(model_path))
                self.model.to(self.device)
                self.model.eval()
                self.model_loaded = True
                self.tokenizer_loaded = True
                logger.info("âœ… å¾®è°ƒæ¨¡å‹åŠ è½½æˆåŠŸ (ç­–ç•¥1)")
                return True
                
            except Exception as e1:
                logger.warning(f"å¾®è°ƒæ¨¡å‹åŠ è½½ç­–ç•¥1å¤±è´¥: {e1}")
                
                # ç­–ç•¥2: åˆ†åˆ«å¤„ç†åˆ†è¯å™¨å’Œæ¨¡å‹
                try:
                    # ä½¿ç”¨åŸºç¡€åˆ†è¯å™¨
                    self.tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base")
                    self.tokenizer_loaded = True
                    
                    # åŠ è½½å¾®è°ƒçš„æ¨¡å‹æƒé‡
                    self.model = AutoModelForSequenceClassification.from_pretrained(str(model_path))
                    self.model.to(self.device)
                    self.model.eval()
                    self.model_loaded = True
                    
                    logger.info("âœ… å¾®è°ƒæ¨¡å‹åŠ è½½æˆåŠŸ (ç­–ç•¥2)")
                    return True
                    
                except Exception as e2:
                    logger.error(f"å¾®è°ƒæ¨¡å‹åŠ è½½ç­–ç•¥2å¤±è´¥: {e2}")
                    raise e2
            
        except Exception as e:
            logger.error(f"âŒ å¾®è°ƒæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # å›é€€åˆ°é¢„è®­ç»ƒæ¨¡å‹
            logger.info("ğŸ”„ å›é€€åˆ°é¢„è®­ç»ƒæ¨¡å‹")
            self._load_pretrained_model_safe()
            return False
    
    def predict_single(self, text: str, return_dict: bool = False) -> Union[np.ndarray, Dict[str, float]]:
        """å…¼å®¹ç‰ˆå•æ–‡æœ¬é¢„æµ‹"""
        try:
            # æ£€æŸ¥æ¨¡å‹çŠ¶æ€
            if not self.model_loaded or not self.tokenizer_loaded:
                logger.warning("âš ï¸ æ¨¡å‹æœªæ­£ç¡®åŠ è½½ï¼Œè¿”å›é›¶å‘é‡")
                zero_vector = np.zeros(27, dtype=np.float32)
                return self.mapper.map_ck_vector_to_dict(zero_vector) if return_dict else zero_vector
            
            if not text or len(text.strip()) < 1:
                logger.warning("âš ï¸ è¾“å…¥æ–‡æœ¬ä¸ºç©ºï¼Œè¿”å›é›¶å‘é‡")
                zero_vector = np.zeros(27, dtype=np.float32)
                return self.mapper.map_ck_vector_to_dict(zero_vector) if return_dict else zero_vector
            
            # æ–‡æœ¬é¢„å¤„ç†å’Œåˆ†è¯
            inputs = self.tokenizer(
                text,
                padding=True,
                truncation=True,
                max_length=MODEL_CONFIG["max_length"],
                return_tensors="pt"
            )
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # æ¨¡å‹æ¨ç†
            self.model.eval()
            with torch.no_grad():
                outputs = self.model(**inputs)
                logits = outputs.logits
                
                # åº”ç”¨sigmoidæ¿€æ´» (å¤šæ ‡ç­¾åˆ†ç±»)
                probabilities = torch.sigmoid(logits).cpu().numpy().flatten()
            
            # ç¡®ä¿è¾“å‡ºç»´åº¦æ­£ç¡®
            if len(probabilities) != 27:
                logger.error(f"âŒ æ¨¡å‹è¾“å‡ºç»´åº¦é”™è¯¯: æœŸæœ›27ç»´ï¼Œå®é™…{len(probabilities)}ç»´")
                probabilities = np.zeros(27, dtype=np.float32)
            
            # åº”ç”¨ç½®ä¿¡åº¦é˜ˆå€¼
            threshold = INFERENCE_CONFIG["confidence_threshold"]
            probabilities = np.where(probabilities > threshold, probabilities, 0.0)
            
            # å½’ä¸€åŒ–åˆ°[0, 1]
            probabilities = np.clip(probabilities, 0, 1)
            
            if return_dict:
                return self.mapper.map_ck_vector_to_dict(probabilities)
            else:
                return probabilities.astype(np.float32)
                
        except Exception as e:
            logger.error(f"âŒ å•æ–‡æœ¬é¢„æµ‹å¤±è´¥: {e}")
            zero_vector = np.zeros(27, dtype=np.float32)
            return self.mapper.map_ck_vector_to_dict(zero_vector) if return_dict else zero_vector
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'model_loaded': self.model_loaded,
            'tokenizer_loaded': self.tokenizer_loaded,
            'device': self.device,
            'model_name': self.model_name,
            'num_labels': self.num_labels
        }

# å…¼å®¹æ€§åŒ…è£…å‡½æ•°
def create_compatible_classifier(**kwargs):
    """åˆ›å»ºå…¼å®¹ç‰ˆåˆ†ç±»å™¨çš„å·¥å‚å‡½æ•°"""
    return CompatibleEmotionClassifier(**kwargs)

if __name__ == "__main__":
    # æµ‹è¯•å…¼å®¹ç‰ˆåˆ†ç±»å™¨
    print("ğŸ§ª æµ‹è¯•å…¼å®¹ç‰ˆæƒ…æ„Ÿåˆ†ç±»å™¨")
    classifier = CompatibleEmotionClassifier(load_pretrained=True)
    
    # å°è¯•åŠ è½½å¾®è°ƒæ¨¡å‹
    try:
        classifier.load_finetuned_model_safe()
    except:
        pass
    
    # æ˜¾ç¤ºçŠ¶æ€
    info = classifier.get_model_info()
    print(f"æ¨¡å‹çŠ¶æ€: {info}")
    
    # æµ‹è¯•é¢„æµ‹
    test_text = "æˆ‘ä»Šå¤©å¾ˆå¼€å¿ƒ"
    result = classifier.predict_single(test_text)
    print(f"æµ‹è¯•ç»“æœ: {result[:5]}... (å‰5ç»´)")
