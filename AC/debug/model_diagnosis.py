#!/usr/bin/env python3
"""
ACæƒ…æ„Ÿåˆ†ææ¨¡å—æ·±åº¦è¯Šæ–­è„šæœ¬

åˆ†ææ¨¡å‹åŠŸèƒ½å¤±æ•ˆçš„æ ¹æœ¬åŸå› ï¼š
1. æ¨¡å‹æƒé‡åŠ è½½éªŒè¯
2. æ¨ç†æµç¨‹å®Œæ•´æ€§æ£€æŸ¥
3. è¾“å‡ºåˆ†å¸ƒå¼‚å¸¸æ£€æµ‹
4. æ¢¯åº¦å’Œæ¿€æ´»å€¼åˆ†æ
5. é…ç½®å…¼å®¹æ€§éªŒè¯
"""

import sys
import os
import torch
import numpy as np
import pandas as pd
import logging
from pathlib import Path
from typing import Dict, Any, List, Tuple

# æ·»åŠ ACæ¨¡å—è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EmotionModelDiagnosis:
    """æƒ…æ„Ÿæ¨¡å‹æ·±åº¦è¯Šæ–­å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è¯Šæ–­å™¨"""
        self.ac_root = Path(__file__).parent.parent
        self.diagnosis_results = {}
        
    def run_complete_diagnosis(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´è¯Šæ–­æµç¨‹"""
        logger.info("ğŸ” å¼€å§‹ACæƒ…æ„Ÿæ¨¡å‹å®Œæ•´è¯Šæ–­...")
        
        # 1. ç¯å¢ƒå’Œé…ç½®æ£€æŸ¥
        self.diagnosis_results['environment'] = self._check_environment()
        
        # 2. æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥
        self.diagnosis_results['model_files'] = self._check_model_files()
        
        # 3. é…ç½®ä¸€è‡´æ€§æ£€æŸ¥
        self.diagnosis_results['config_consistency'] = self._check_config_consistency()
        
        # 4. æ¨¡å‹åŠ è½½å’Œæƒé‡æ£€æŸ¥
        self.diagnosis_results['model_loading'] = self._check_model_loading()
        
        # 5. æ¨ç†æµç¨‹è¯Šæ–­
        self.diagnosis_results['inference_flow'] = self._check_inference_flow()
        
        # 6. è¾“å‡ºåˆ†å¸ƒåˆ†æ
        self.diagnosis_results['output_analysis'] = self._analyze_model_outputs()
        
        # 7. æƒé‡å’Œæ¢¯åº¦åˆ†æ
        self.diagnosis_results['weight_analysis'] = self._analyze_model_weights()
        
        # 8. ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
        report = self._generate_diagnosis_report()
        
        return report
    
    def _check_environment(self) -> Dict[str, Any]:
        """æ£€æŸ¥ç¯å¢ƒé…ç½®"""
        logger.info("ğŸ“‹ æ£€æŸ¥ç¯å¢ƒé…ç½®...")
        
        env_info = {
            'python_version': sys.version,
            'torch_version': torch.__version__,
            'cuda_available': torch.cuda.is_available(),
            'mps_available': hasattr(torch.backends, 'mps') and torch.backends.mps.is_available(),
            'device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0
        }
        
        # æ£€æŸ¥transformersç‰ˆæœ¬
        try:
            import transformers
            env_info['transformers_version'] = transformers.__version__
        except ImportError:
            env_info['transformers_version'] = 'Not installed'
        
        # æ£€æŸ¥numpyç‰ˆæœ¬
        env_info['numpy_version'] = np.__version__
        
        logger.info(f"   PyTorch: {env_info['torch_version']}")
        logger.info(f"   Transformers: {env_info['transformers_version']}")
        logger.info(f"   CUDAå¯ç”¨: {env_info['cuda_available']}")
        logger.info(f"   MPSå¯ç”¨: {env_info['mps_available']}")
        
        return env_info
    
    def _check_model_files(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§"""
        logger.info("ğŸ“ æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§...")
        
        model_dir = self.ac_root / "models" / "finetuned_xlm_roberta"
        
        required_files = [
            'config.json',
            'model.safetensors', 
            'tokenizer.json',
            'tokenizer_config.json',
            'sentencepiece.bpe.model'
        ]
        
        file_status = {}
        total_size = 0
        
        for filename in required_files:
            filepath = model_dir / filename
            if filepath.exists():
                size = filepath.stat().st_size
                file_status[filename] = {
                    'exists': True,
                    'size_bytes': size,
                    'size_mb': round(size / (1024*1024), 2)
                }
                total_size += size
            else:
                file_status[filename] = {'exists': False, 'size_bytes': 0}
        
        result = {
            'model_directory': str(model_dir),
            'directory_exists': model_dir.exists(),
            'file_status': file_status,
            'total_size_mb': round(total_size / (1024*1024), 2),
            'all_files_present': all(status['exists'] for status in file_status.values())
        }
        
        logger.info(f"   æ¨¡å‹ç›®å½•: {model_dir}")
        logger.info(f"   æ€»å¤§å°: {result['total_size_mb']} MB")
        logger.info(f"   å®Œæ•´æ€§: {'âœ… å®Œæ•´' if result['all_files_present'] else 'âŒ ç¼ºå¤±æ–‡ä»¶'}")
        
        return result
    
    def _check_config_consistency(self) -> Dict[str, Any]:
        """æ£€æŸ¥é…ç½®ä¸€è‡´æ€§"""
        logger.info("âš™ï¸  æ£€æŸ¥é…ç½®ä¸€è‡´æ€§...")
        
        result = {}
        
        # æ£€æŸ¥ACæ¨¡å—é…ç½®
        try:
            from config import MODEL_CONFIG, COWEN_KELTNER_EMOTIONS, MODEL_PATHS
            result['ac_config'] = {
                'model_name': MODEL_CONFIG['model_name'],
                'num_labels': MODEL_CONFIG['num_labels'],
                'max_length': MODEL_CONFIG['max_length'],
                'ck_emotions_count': len(COWEN_KELTNER_EMOTIONS),
                'model_path': str(MODEL_PATHS['finetuned_model'])
            }
        except Exception as e:
            result['ac_config_error'] = str(e)
        
        # æ£€æŸ¥å®é™…æ¨¡å‹é…ç½®
        try:
            import json
            model_config_path = self.ac_root / "models" / "finetuned_xlm_roberta" / "config.json"
            if model_config_path.exists():
                with open(model_config_path) as f:
                    model_config = json.load(f)
                
                result['model_config'] = {
                    'architectures': model_config.get('architectures', []),
                    'num_labels': len(model_config.get('id2label', {})),
                    'problem_type': model_config.get('problem_type'),
                    'hidden_size': model_config.get('hidden_size'),
                    'vocab_size': model_config.get('vocab_size')
                }
        except Exception as e:
            result['model_config_error'] = str(e)
        
        # ä¸€è‡´æ€§æ£€æŸ¥
        if 'ac_config' in result and 'model_config' in result:
            result['consistency_check'] = {
                'num_labels_match': result['ac_config']['num_labels'] == result['model_config']['num_labels'],
                'architecture_correct': 'XLMRobertaForSequenceClassification' in result['model_config']['architectures'],
                'problem_type_correct': result['model_config']['problem_type'] == 'multi_label_classification'
            }
        
        logger.info(f"   é…ç½®æ¨¡å‹æ ‡ç­¾æ•°: {result.get('ac_config', {}).get('num_labels', 'N/A')}")
        logger.info(f"   å®é™…æ¨¡å‹æ ‡ç­¾æ•°: {result.get('model_config', {}).get('num_labels', 'N/A')}")
        
        return result
    
    def _check_model_loading(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ¨¡å‹åŠ è½½è¿‡ç¨‹"""
        logger.info("ğŸ”„ æ£€æŸ¥æ¨¡å‹åŠ è½½è¿‡ç¨‹...")
        
        result = {}
        
        try:
            # å°è¯•åŠ è½½åˆ†è¯å™¨
            from transformers import AutoTokenizer
            model_path = self.ac_root / "models" / "finetuned_xlm_roberta"
            
            tokenizer = AutoTokenizer.from_pretrained(str(model_path))
            result['tokenizer_loading'] = {
                'success': True,
                'vocab_size': tokenizer.vocab_size,
                'model_max_length': tokenizer.model_max_length
            }
            logger.info("   âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            result['tokenizer_loading'] = {
                'success': False,
                'error': str(e)
            }
            logger.error(f"   âŒ åˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}")
        
        try:
            # å°è¯•åŠ è½½æ¨¡å‹
            from transformers import AutoModelForSequenceClassification
            model_path = self.ac_root / "models" / "finetuned_xlm_roberta"
            
            model = AutoModelForSequenceClassification.from_pretrained(str(model_path))
            
            result['model_loading'] = {
                'success': True,
                'num_parameters': sum(p.numel() for p in model.parameters()),
                'model_dtype': str(model.dtype),
                'classifier_out_features': model.classifier.out_features if hasattr(model, 'classifier') else 'N/A'
            }
            
            # æ£€æŸ¥åˆ†ç±»å™¨å±‚ç»“æ„
            if hasattr(model, 'classifier'):
                classifier = model.classifier
                result['classifier_structure'] = {
                    'type': type(classifier).__name__,
                    'in_features': getattr(classifier, 'in_features', 'N/A'),
                    'out_features': getattr(classifier, 'out_features', 'N/A')
                }
            
            logger.info("   âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            logger.info(f"   å‚æ•°æ•°é‡: {result['model_loading']['num_parameters']:,}")
            
            # ä¿å­˜æ¨¡å‹å¼•ç”¨ç”¨äºåç»­æµ‹è¯•
            self.loaded_model = model
            self.loaded_tokenizer = tokenizer
            
        except Exception as e:
            result['model_loading'] = {
                'success': False,
                'error': str(e)
            }
            logger.error(f"   âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        
        return result
    
    def _check_inference_flow(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ¨ç†æµç¨‹"""
        logger.info("ğŸ§  æ£€æŸ¥æ¨ç†æµç¨‹...")
        
        result = {}
        
        if not hasattr(self, 'loaded_model') or not hasattr(self, 'loaded_tokenizer'):
            result['error'] = "æ¨¡å‹æˆ–åˆ†è¯å™¨æœªåŠ è½½"
            return result
        
        test_texts = [
            "æˆ‘ä»Šå¤©å¾ˆå¼€å¿ƒ",
            "I am feeling sad", 
            "This is frustrating",
            "",  # ç©ºæ–‡æœ¬æµ‹è¯•
            "a" * 1000  # é•¿æ–‡æœ¬æµ‹è¯•
        ]
        
        inference_results = []
        
        for i, text in enumerate(test_texts):
            try:
                # åˆ†è¯
                inputs = self.loaded_tokenizer(
                    text,
                    padding=True,
                    truncation=True,
                    max_length=512,
                    return_tensors="pt"
                )
                
                # æ¨ç†
                self.loaded_model.eval()
                with torch.no_grad():
                    outputs = self.loaded_model(**inputs)
                    logits = outputs.logits
                    probs = torch.sigmoid(logits)
                
                # åˆ†æè¾“å‡º
                test_result = {
                    'text_length': len(text),
                    'input_ids_shape': inputs['input_ids'].shape,
                    'logits_shape': logits.shape,
                    'logits_mean': float(logits.mean()),
                    'logits_std': float(logits.std()),
                    'logits_min': float(logits.min()),
                    'logits_max': float(logits.max()),
                    'probs_mean': float(probs.mean()),
                    'probs_std': float(probs.std()),
                    'probs_sum': float(probs.sum()),
                    'active_outputs': int(torch.sum(probs > 0.1)),
                    'logits_sample': logits[0][:5].tolist(),  # å‰5ä¸ªlogits
                    'probs_sample': probs[0][:5].tolist()    # å‰5ä¸ªæ¦‚ç‡
                }
                
                inference_results.append(test_result)
                
            except Exception as e:
                inference_results.append({
                    'text_length': len(text),
                    'error': str(e)
                })
        
        result['test_results'] = inference_results
        
        # æ£€æŸ¥è¾“å‡ºå¼‚å¸¸æ¨¡å¼
        valid_results = [r for r in inference_results if 'error' not in r]
        if valid_results:
            logits_means = [r['logits_mean'] for r in valid_results]
            probs_means = [r['probs_mean'] for r in valid_results]
            
            result['output_patterns'] = {
                'logits_variance': float(np.var(logits_means)),
                'probs_variance': float(np.var(probs_means)),
                'consistent_outputs': np.var(logits_means) < 0.01,  # è¾“å‡ºè¿‡äºä¸€è‡´
                'avg_active_outputs': np.mean([r['active_outputs'] for r in valid_results])
            }
        
        logger.info(f"   æµ‹è¯•æ ·æœ¬: {len(test_texts)}")
        logger.info(f"   æˆåŠŸæ¨ç†: {len(valid_results)}")
        if 'output_patterns' in result:
            logger.info(f"   è¾“å‡ºä¸€è‡´æ€§: {'âŒ å¼‚å¸¸' if result['output_patterns']['consistent_outputs'] else 'âœ… æ­£å¸¸'}")
        
        return result
    
    def _analyze_model_outputs(self) -> Dict[str, Any]:
        """åˆ†ææ¨¡å‹è¾“å‡ºåˆ†å¸ƒ"""
        logger.info("ğŸ“Š åˆ†ææ¨¡å‹è¾“å‡ºåˆ†å¸ƒ...")
        
        result = {}
        
        if not hasattr(self, 'loaded_model') or not hasattr(self, 'loaded_tokenizer'):
            result['error'] = "æ¨¡å‹æˆ–åˆ†è¯å™¨æœªåŠ è½½"
            return result
        
        # å‡†å¤‡å¤šæ ·åŒ–æµ‹è¯•æ•°æ®
        diverse_texts = [
            # æ˜ç¡®çš„æƒ…æ„Ÿè¡¨è¾¾
            "I am extremely happy and joyful today!",
            "æˆ‘æ„Ÿåˆ°éå¸¸æ„¤æ€’å’Œç”Ÿæ°”",
            "This makes me incredibly sad and depressed",
            "I feel anxious and worried about tomorrow",
            "è¿™éŸ³ä¹è®©æˆ‘æ„Ÿåˆ°å¹³é™å’Œæ”¾æ¾",
            
            # æ··åˆæƒ…æ„Ÿ
            "I'm excited but also nervous about the presentation",
            "Happy memories mixed with sadness",
            
            # ä¸­æ€§æ–‡æœ¬
            "The weather is cloudy today",
            "Please pass the salt",
            "Technical documentation for the API",
            
            # ä¸åŒè¯­è¨€
            "Je suis trÃ¨s heureux aujourd'hui",
            "Estoy muy triste por las noticias",
            
            # æç«¯æƒ…å†µ
            "!!!AMAZING WONDERFUL FANTASTIC!!!",
            "terrible awful horrible disgusting",
            "okay fine whatever sure"
        ]
        
        all_outputs = []
        all_logits = []
        
        self.loaded_model.eval()
        
        for text in diverse_texts:
            try:
                inputs = self.loaded_tokenizer(
                    text,
                    padding=True,
                    truncation=True,
                    max_length=512,
                    return_tensors="pt"
                )
                
                with torch.no_grad():
                    outputs = self.loaded_model(**inputs)
                    logits = outputs.logits[0]  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬
                    probs = torch.sigmoid(logits)
                
                all_outputs.append(probs.numpy())
                all_logits.append(logits.numpy())
                
            except Exception as e:
                logger.warning(f"å¤„ç†æ–‡æœ¬å¤±è´¥: {text[:30]}... -> {e}")
        
        if all_outputs:
            outputs_array = np.array(all_outputs)  # (N, 27)
            logits_array = np.array(all_logits)    # (N, 27)
            
            # åˆ†æè¾“å‡ºåˆ†å¸ƒ
            result['distribution_analysis'] = {
                'num_samples': len(all_outputs),
                'output_shape': outputs_array.shape,
                
                # æŒ‰æ ·æœ¬åˆ†æ
                'sample_statistics': {
                    'mean_per_sample': outputs_array.mean(axis=1).tolist(),
                    'std_per_sample': outputs_array.std(axis=1).tolist(),
                    'max_per_sample': outputs_array.max(axis=1).tolist(),
                    'active_emotions_per_sample': (outputs_array > 0.1).sum(axis=1).tolist()
                },
                
                # æŒ‰ç»´åº¦åˆ†æ
                'dimension_statistics': {
                    'mean_per_dim': outputs_array.mean(axis=0).tolist(),
                    'std_per_dim': outputs_array.std(axis=0).tolist(),
                    'activation_rate_per_dim': (outputs_array > 0.1).mean(axis=0).tolist()
                },
                
                # æ•´ä½“ç»Ÿè®¡
                'overall_statistics': {
                    'global_mean': float(outputs_array.mean()),
                    'global_std': float(outputs_array.std()),
                    'zero_ratio': float((outputs_array == 0).mean()),
                    'low_activation_ratio': float((outputs_array < 0.01).mean())
                },
                
                # logitsåˆ†æ
                'logits_statistics': {
                    'mean': float(logits_array.mean()),
                    'std': float(logits_array.std()),
                    'min': float(logits_array.min()),
                    'max': float(logits_array.max())
                }
            }
            
            # æ£€æµ‹å¼‚å¸¸æ¨¡å¼
            result['anomaly_detection'] = {
                'all_outputs_identical': np.allclose(outputs_array, outputs_array[0], atol=1e-6),
                'extremely_low_variance': outputs_array.std() < 1e-4,
                'dominated_by_zeros': (outputs_array == 0).mean() > 0.9,
                'single_dimension_dominance': outputs_array.max(axis=1).mean() > 0.9,
                'logits_collapsed': abs(logits_array.std()) < 0.1
            }
            
            logger.info(f"   åˆ†ææ ·æœ¬æ•°: {len(all_outputs)}")
            logger.info(f"   å…¨å±€å‡å€¼: {result['distribution_analysis']['overall_statistics']['global_mean']:.4f}")
            logger.info(f"   å…¨å±€æ ‡å‡†å·®: {result['distribution_analysis']['overall_statistics']['global_std']:.4f}")
            
            # å¼‚å¸¸æ£€æµ‹ç»“æœ
            anomalies = [k for k, v in result['anomaly_detection'].items() if v]
            if anomalies:
                logger.warning(f"   âš ï¸ æ£€æµ‹åˆ°å¼‚å¸¸: {anomalies}")
            else:
                logger.info("   âœ… è¾“å‡ºåˆ†å¸ƒæ­£å¸¸")
        
        return result
    
    def _analyze_model_weights(self) -> Dict[str, Any]:
        """åˆ†ææ¨¡å‹æƒé‡"""
        logger.info("âš–ï¸  åˆ†ææ¨¡å‹æƒé‡...")
        
        result = {}
        
        if not hasattr(self, 'loaded_model'):
            result['error'] = "æ¨¡å‹æœªåŠ è½½"
            return result
        
        try:
            # åˆ†æåˆ†ç±»å™¨æƒé‡
            if hasattr(self.loaded_model, 'classifier'):
                classifier = self.loaded_model.classifier
                
                if hasattr(classifier, 'weight'):
                    weight = classifier.weight.data
                    bias = classifier.bias.data if hasattr(classifier, 'bias') else None
                    
                    result['classifier_weights'] = {
                        'weight_shape': list(weight.shape),
                        'weight_mean': float(weight.mean()),
                        'weight_std': float(weight.std()),
                        'weight_min': float(weight.min()),
                        'weight_max': float(weight.max()),
                        'weight_zeros': int((weight == 0).sum()),
                        'weight_norm': float(weight.norm()),
                    }
                    
                    if bias is not None:
                        result['classifier_bias'] = {
                            'bias_shape': list(bias.shape),
                            'bias_mean': float(bias.mean()),
                            'bias_std': float(bias.std()),
                            'bias_min': float(bias.min()),
                            'bias_max': float(bias.max())
                        }
                    
                    # æ£€æŸ¥æƒé‡æ˜¯å¦è¢«æ­£ç¡®åˆå§‹åŒ–/å¾®è°ƒ
                    result['weight_analysis'] = {
                        'weights_initialized': not torch.allclose(weight, torch.zeros_like(weight)),
                        'reasonable_scale': 0.001 < weight.std() < 10.0,
                        'symmetric_distribution': abs(weight.mean()) < weight.std(),
                    }
            
            # æ£€æŸ¥ä¸€äº›å…³é”®å±‚çš„æƒé‡ç»Ÿè®¡
            layer_stats = {}
            for name, param in self.loaded_model.named_parameters():
                if 'classifier' in name or 'pooler' in name:
                    layer_stats[name] = {
                        'shape': list(param.shape),
                        'mean': float(param.mean()),
                        'std': float(param.std()),
                        'requires_grad': param.requires_grad
                    }
            
            result['key_layers'] = layer_stats
            
            logger.info("   âœ… æƒé‡åˆ†æå®Œæˆ")
            if 'classifier_weights' in result:
                logger.info(f"   åˆ†ç±»å™¨æƒé‡å½¢çŠ¶: {result['classifier_weights']['weight_shape']}")
                logger.info(f"   æƒé‡æ ‡å‡†å·®: {result['classifier_weights']['weight_std']:.6f}")
            
        except Exception as e:
            result['error'] = str(e)
            logger.error(f"   âŒ æƒé‡åˆ†æå¤±è´¥: {e}")
        
        return result
    
    def _generate_diagnosis_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š"""
        logger.info("ğŸ“‹ ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š...")
        
        # ç»¼åˆåˆ†æé—®é¢˜
        issues = []
        recommendations = []
        severity_score = 0  # 0-100, 100æœ€ä¸¥é‡
        
        # æ£€æŸ¥å„ä¸ªè¯Šæ–­ç»“æœ
        if not self.diagnosis_results.get('model_files', {}).get('all_files_present', False):
            issues.append("âŒ æ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´")
            recommendations.append("é‡æ–°ä¸‹è½½æˆ–è®­ç»ƒæ¨¡å‹æ–‡ä»¶")
            severity_score += 30
        
        config_check = self.diagnosis_results.get('config_consistency', {}).get('consistency_check', {})
        if not config_check.get('num_labels_match', True):
            issues.append("âŒ é…ç½®æ ‡ç­¾æ•°ä¸åŒ¹é…")
            recommendations.append("æ£€æŸ¥config.pyä¸­çš„æ ‡ç­¾æ•°è®¾ç½®")
            severity_score += 25
        
        if not config_check.get('problem_type_correct', True):
            issues.append("âŒ é—®é¢˜ç±»å‹é…ç½®é”™è¯¯")
            recommendations.append("ç¡®ä¿æ¨¡å‹é…ç½®ä¸ºå¤šæ ‡ç­¾åˆ†ç±»")
            severity_score += 20
        
        # æ£€æŸ¥æ¨¡å‹åŠ è½½
        if not self.diagnosis_results.get('model_loading', {}).get('model_loading', {}).get('success', False):
            issues.append("âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
            recommendations.append("æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ ¼å¼å’Œtransformersç‰ˆæœ¬")
            severity_score += 40
        
        # æ£€æŸ¥æ¨ç†å¼‚å¸¸
        inference_patterns = self.diagnosis_results.get('inference_flow', {}).get('output_patterns', {})
        if inference_patterns.get('consistent_outputs', False):
            issues.append("âŒ æ¨¡å‹è¾“å‡ºè¿‡äºä¸€è‡´ï¼Œå¯èƒ½æœªæ­£ç¡®åŠ è½½æƒé‡")
            recommendations.append("æ£€æŸ¥æ¨¡å‹æƒé‡æ˜¯å¦æ­£ç¡®åŠ è½½å’Œåˆå§‹åŒ–")
            severity_score += 35
        
        # æ£€æŸ¥è¾“å‡ºåˆ†å¸ƒå¼‚å¸¸
        anomalies = self.diagnosis_results.get('output_analysis', {}).get('anomaly_detection', {})
        if anomalies.get('all_outputs_identical', False):
            issues.append("âŒ æ‰€æœ‰è¾“å…¥äº§ç”Ÿç›¸åŒè¾“å‡º - æƒé‡åŠ è½½é—®é¢˜")
            recommendations.append("é‡æ–°åŠ è½½æˆ–é‡æ–°è®­ç»ƒæ¨¡å‹")
            severity_score += 50
        
        if anomalies.get('logits_collapsed', False):
            issues.append("âŒ Logitsåˆ†å¸ƒåå¡Œ - æ¨¡å‹é€€åŒ–")
            recommendations.append("æ£€æŸ¥è®­ç»ƒè¿‡ç¨‹æˆ–ä½¿ç”¨å¤‡ä»½æ¨¡å‹")
            severity_score += 45
        
        # æƒé‡åˆ†æ
        weight_analysis = self.diagnosis_results.get('weight_analysis', {}).get('weight_analysis', {})
        if not weight_analysis.get('weights_initialized', True):
            issues.append("âŒ åˆ†ç±»å™¨æƒé‡æœªåˆå§‹åŒ–")
            recommendations.append("æ£€æŸ¥æ¨¡å‹ä¿å­˜å’ŒåŠ è½½è¿‡ç¨‹")
            severity_score += 40
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        report = {
            'diagnosis_summary': {
                'total_issues': len(issues),
                'severity_score': min(severity_score, 100),
                'status': 'CRITICAL' if severity_score > 70 else 'WARNING' if severity_score > 30 else 'OK',
                'primary_issues': issues[:5],  # æœ€é‡è¦çš„5ä¸ªé—®é¢˜
                'recommendations': recommendations[:5]
            },
            'detailed_results': self.diagnosis_results,
            'conclusion': self._generate_conclusion(issues, severity_score)
        }
        
        # è¾“å‡ºæŠ¥å‘Šæ‘˜è¦
        logger.info("="*60)
        logger.info("ğŸ¯ è¯Šæ–­æŠ¥å‘Šæ‘˜è¦")
        logger.info("="*60)
        logger.info(f"ä¸¥é‡ç¨‹åº¦: {report['diagnosis_summary']['status']} (è¯„åˆ†: {severity_score}/100)")
        logger.info(f"å‘ç°é—®é¢˜: {len(issues)} ä¸ª")
        
        for i, issue in enumerate(issues[:3], 1):
            logger.info(f"{i}. {issue}")
        
        logger.info("\nğŸ’¡ ä¸»è¦å»ºè®®:")
        for i, rec in enumerate(recommendations[:3], 1):
            logger.info(f"{i}. {rec}")
        
        return report
    
    def _generate_conclusion(self, issues: List[str], severity_score: int) -> str:
        """ç”Ÿæˆè¯Šæ–­ç»“è®º"""
        if severity_score > 70:
            return "æ¨¡å‹å­˜åœ¨ä¸¥é‡é—®é¢˜ï¼Œéœ€è¦ç«‹å³ä¿®å¤ã€‚ä¸»è¦é—®é¢˜é›†ä¸­åœ¨æ¨¡å‹æƒé‡åŠ è½½å’Œé…ç½®ä¸åŒ¹é…ã€‚"
        elif severity_score > 30:
            return "æ¨¡å‹å­˜åœ¨ä¸­ç­‰é—®é¢˜ï¼Œå»ºè®®ä¼˜å…ˆä¿®å¤æƒé‡å’Œé…ç½®ç›¸å…³é—®é¢˜ã€‚"
        else:
            return "æ¨¡å‹æ•´ä½“çŠ¶æ€è‰¯å¥½ï¼Œå¯èƒ½å­˜åœ¨è½»å¾®è°ƒä¼˜ç©ºé—´ã€‚"
    
    def save_diagnosis_report(self, report: Dict[str, Any], filename: str = "diagnosis_report.json"):
        """ä¿å­˜è¯Šæ–­æŠ¥å‘Š"""
        output_path = Path(__file__).parent / filename
        
        import json
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“„ è¯Šæ–­æŠ¥å‘Šå·²ä¿å­˜: {output_path}")

def main():
    """è¿è¡Œå®Œæ•´è¯Šæ–­"""
    print("ğŸ” ACæƒ…æ„Ÿåˆ†ææ¨¡å—æ·±åº¦è¯Šæ–­")
    print("="*60)
    
    # åˆ›å»ºè¯Šæ–­å™¨
    diagnosis = EmotionModelDiagnosis()
    
    # è¿è¡Œå®Œæ•´è¯Šæ–­
    report = diagnosis.run_complete_diagnosis()
    
    # ä¿å­˜æŠ¥å‘Š
    diagnosis.save_diagnosis_report(report)
    
    print("\n" + "="*60)
    print("âœ… è¯Šæ–­å®Œæˆï¼è¯·æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Šã€‚")
    
    return report

if __name__ == "__main__":
    main()