#!/usr/bin/env python3
"""
ACæ¨¡å—ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜è¯Šæ–­å’Œä¿®å¤è„šæœ¬

ä¸»è¦é—®é¢˜åˆ†æï¼š
1. tokenizer.jsonæ ¼å¼ä¸å½“å‰transformersç‰ˆæœ¬ä¸å…¼å®¹
2. æ¨¡å‹ç»“æ„è®¿é—®æ–¹å¼åœ¨æ–°ç‰ˆæœ¬ä¸­å‘ç”Ÿå˜åŒ–
3. éœ€è¦é€‚é…ä¸åŒç‰ˆæœ¬çš„transformersåº“

ä¿®å¤ç­–ç•¥ï¼š
1. é‡æ–°æ„å»ºå…¼å®¹çš„åˆ†è¯å™¨é…ç½®
2. ä¿®å¤æ¨¡å‹ç»“æ„è®¿é—®é€»è¾‘
3. æä¾›ç‰ˆæœ¬é€‚é…ä»£ç 
"""

import sys
import os
import torch
import json
import logging
from pathlib import Path
from typing import Dict, Any, Optional, Tuple

# æ·»åŠ ACæ¨¡å—è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class VersionCompatibilityFixer:
    """ç‰ˆæœ¬å…¼å®¹æ€§ä¿®å¤å™¨"""
    
    def __init__(self):
        self.ac_root = Path(__file__).parent.parent
        self.model_dir = self.ac_root / "models" / "finetuned_xlm_roberta"
        
    def diagnose_version_issues(self) -> Dict[str, Any]:
        """è¯Šæ–­ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜"""
        logger.info("ğŸ” è¯Šæ–­ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜...")
        
        issues = {
            'transformers_version': self._check_transformers_version(),
            'tokenizer_compatibility': self._check_tokenizer_compatibility(),
            'model_structure_compatibility': self._check_model_structure(),
            'recommended_fixes': []
        }
        
        return issues
    
    def _check_transformers_version(self) -> Dict[str, Any]:
        """æ£€æŸ¥transformersç‰ˆæœ¬"""
        try:
            import transformers
            version = transformers.__version__
            
            # è§£æç‰ˆæœ¬å·
            major, minor, patch = map(int, version.split('.'))
            
            result = {
                'current_version': version,
                'major': major,
                'minor': minor, 
                'patch': patch,
                'compatible': True,
                'issues': []
            }
            
            # æ£€æŸ¥å·²çŸ¥é—®é¢˜ç‰ˆæœ¬
            if major == 4 and minor < 21:
                result['issues'].append("ç‰ˆæœ¬è¿‡ä½ï¼Œå»ºè®®å‡çº§åˆ°4.21+")
                result['compatible'] = False
            
            if major == 4 and minor >= 30:
                result['issues'].append("tokenizeræ ¼å¼å¯èƒ½ä¸å…¼å®¹ï¼Œéœ€è¦é‡æ–°ç”Ÿæˆ")
            
            return result
            
        except Exception as e:
            return {'error': str(e), 'compatible': False}
    
    def _check_tokenizer_compatibility(self) -> Dict[str, Any]:
        """æ£€æŸ¥åˆ†è¯å™¨å…¼å®¹æ€§"""
        logger.info("ğŸ”¤ æ£€æŸ¥åˆ†è¯å™¨å…¼å®¹æ€§...")
        
        result = {'tests': []}
        
        # æµ‹è¯•1: ç›´æ¥åŠ è½½åŸå§‹åˆ†è¯å™¨
        try:
            from transformers import AutoTokenizer
            tokenizer = AutoTokenizer.from_pretrained(str(self.model_dir))
            result['tests'].append({
                'name': 'direct_loading',
                'success': True,
                'vocab_size': tokenizer.vocab_size
            })
        except Exception as e:
            result['tests'].append({
                'name': 'direct_loading',
                'success': False,
                'error': str(e)
            })
        
        # æµ‹è¯•2: ä½¿ç”¨é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹
        try:
            from transformers import AutoTokenizer
            tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base")
            result['tests'].append({
                'name': 'base_model_loading',
                'success': True,
                'vocab_size': tokenizer.vocab_size
            })
            # ä¿å­˜åŸºç¡€åˆ†è¯å™¨å¼•ç”¨
            self._base_tokenizer = tokenizer
        except Exception as e:
            result['tests'].append({
                'name': 'base_model_loading', 
                'success': False,
                'error': str(e)
            })
        
        # æµ‹è¯•3: æ£€æŸ¥tokenizeré…ç½®æ–‡ä»¶
        try:
            config_file = self.model_dir / "tokenizer_config.json"
            with open(config_file) as f:
                config = json.load(f)
            result['config_analysis'] = {
                'model_max_length': config.get('model_max_length'),
                'tokenizer_class': config.get('tokenizer_class'),
                'special_tokens': len(config.get('special_tokens_map', {}))
            }
        except Exception as e:
            result['config_error'] = str(e)
        
        return result
    
    def _check_model_structure(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ¨¡å‹ç»“æ„å…¼å®¹æ€§"""
        logger.info("ğŸ—ï¸  æ£€æŸ¥æ¨¡å‹ç»“æ„å…¼å®¹æ€§...")
        
        result = {'tests': []}
        
        # æµ‹è¯•1: å°è¯•åŠ è½½æ¨¡å‹é…ç½®
        try:
            from transformers import AutoConfig
            config = AutoConfig.from_pretrained(str(self.model_dir))
            result['config_loading'] = {
                'success': True,
                'architectures': config.architectures,
                'num_labels': config.num_labels,
                'problem_type': config.problem_type
            }
        except Exception as e:
            result['config_loading'] = {
                'success': False,
                'error': str(e)
            }
        
        # æµ‹è¯•2: å°è¯•åŠ è½½æ¨¡å‹ç»“æ„
        try:
            from transformers import AutoModelForSequenceClassification
            model = AutoModelForSequenceClassification.from_pretrained(str(self.model_dir))
            
            # æ£€æŸ¥åˆ†ç±»å™¨ç»“æ„
            classifier_info = {}
            if hasattr(model, 'classifier'):
                classifier = model.classifier
                classifier_info['has_classifier'] = True
                classifier_info['classifier_type'] = type(classifier).__name__
                
                # å°è¯•ä¸åŒçš„å±æ€§è®¿é—®æ–¹å¼
                for attr in ['out_features', 'out_proj', 'dense']:
                    if hasattr(classifier, attr):
                        attr_val = getattr(classifier, attr)
                        if hasattr(attr_val, 'out_features'):
                            classifier_info[f'{attr}_out_features'] = attr_val.out_features
                        classifier_info[attr] = str(type(attr_val))
            
            result['model_structure'] = {
                'success': True,
                'model_type': type(model).__name__,
                'classifier_info': classifier_info,
                'num_parameters': sum(p.numel() for p in model.parameters())
            }
            
            # ä¿å­˜æ¨¡å‹å¼•ç”¨
            self._loaded_model = model
            
        except Exception as e:
            result['model_structure'] = {
                'success': False,
                'error': str(e)
            }
        
        return result
    
    def fix_tokenizer_compatibility(self) -> bool:
        """ä¿®å¤åˆ†è¯å™¨å…¼å®¹æ€§é—®é¢˜"""
        logger.info("ğŸ”§ ä¿®å¤åˆ†è¯å™¨å…¼å®¹æ€§...")
        
        try:
            # ç­–ç•¥1: ä½¿ç”¨åŸºç¡€æ¨¡å‹åˆ†è¯å™¨å¹¶ä¿å­˜åˆ°æ¨¡å‹ç›®å½•
            if hasattr(self, '_base_tokenizer'):
                backup_dir = self.model_dir / "tokenizer_backup"
                backup_dir.mkdir(exist_ok=True)
                
                # å¤‡ä»½åŸå§‹æ–‡ä»¶
                for file in ['tokenizer.json', 'tokenizer_config.json']:
                    src = self.model_dir / file
                    dst = backup_dir / file
                    if src.exists():
                        import shutil
                        shutil.copy2(src, dst)
                        logger.info(f"å¤‡ä»½ {file} åˆ° {dst}")
                
                # ä¿å­˜å…¼å®¹çš„åˆ†è¯å™¨
                self._base_tokenizer.save_pretrained(str(self.model_dir))
                logger.info("âœ… åˆ†è¯å™¨å…¼å®¹æ€§ä¿®å¤å®Œæˆ")
                
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"âŒ åˆ†è¯å™¨ä¿®å¤å¤±è´¥: {e}")
            return False
    
    def create_compatible_emotion_classifier(self) -> bool:
        """åˆ›å»ºç‰ˆæœ¬å…¼å®¹çš„æƒ…æ„Ÿåˆ†ç±»å™¨"""
        logger.info("ğŸ¯ åˆ›å»ºå…¼å®¹çš„æƒ…æ„Ÿåˆ†ç±»å™¨...")
        
        try:
            # åˆ›å»ºå…¼å®¹ç‰ˆæœ¬çš„emotion_classifier
            compatible_code = '''#!/usr/bin/env python3
"""
ç‰ˆæœ¬å…¼å®¹çš„æƒ…æ„Ÿåˆ†ç±»å™¨ - ä¿®å¤ç‰ˆ

ä¸»è¦ä¿®å¤:
1. åˆ†è¯å™¨åŠ è½½å…¼å®¹æ€§
2. æ¨¡å‹ç»“æ„è®¿é—®é€‚é…
3. è®¾å¤‡æ£€æµ‹ä¼˜åŒ–
4. é”™è¯¯å¤„ç†å¢å¼º
"""

import torch
import torch.nn as nn
import numpy as np
import logging
from typing import Dict, List, Union, Tuple, Optional
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    AutoConfig
)

try:
    from .config import MODEL_CONFIG, MODEL_PATHS, COWEN_KELTNER_EMOTIONS, INFERENCE_CONFIG
    from .emotion_mapper import GoEmotionsMapper
except ImportError:
    from config import MODEL_CONFIG, MODEL_PATHS, COWEN_KELTNER_EMOTIONS, INFERENCE_CONFIG
    from emotion_mapper import GoEmotionsMapper

logger = logging.getLogger(__name__)

class CompatibleEmotionClassifier(nn.Module):
    """ç‰ˆæœ¬å…¼å®¹çš„æƒ…æ„Ÿåˆ†ç±»å™¨"""
    
    def __init__(self, model_name: str = None, num_labels: int = 27, load_pretrained: bool = True):
        """åˆå§‹åŒ–å…¼å®¹ç‰ˆåˆ†ç±»å™¨"""
        super().__init__()
        
        self.model_name = model_name or MODEL_CONFIG["model_name"]
        self.num_labels = num_labels
        self.emotion_names = COWEN_KELTNER_EMOTIONS
        
        # è®¾å¤‡æ£€æµ‹ - å¢å¼ºå…¼å®¹æ€§
        self.device = self._detect_device()
        logger.info(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # æ¨¡å‹åŠ è½½æ ‡å¿—
        self.model_loaded = False
        self.tokenizer_loaded = False
        
        # åˆå§‹åŒ–æ¨¡å‹
        if load_pretrained:
            self._load_pretrained_model_safe()
        
        # åˆå§‹åŒ–æ˜ å°„å™¨
        self.mapper = GoEmotionsMapper()
        
        logger.info("âœ… å…¼å®¹ç‰ˆæƒ…æ„Ÿåˆ†ç±»å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _detect_device(self) -> str:
        """å¢å¼ºçš„è®¾å¤‡æ£€æµ‹"""
        if torch.cuda.is_available():
            return "cuda"
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            # æ£€æŸ¥MPSå®é™…å¯ç”¨æ€§
            try:
                test_tensor = torch.randn(1).to("mps")
                return "mps"
            except:
                return "cpu"
        else:
            return "cpu"
    
    def _load_pretrained_model_safe(self):
        """å®‰å…¨çš„é¢„è®­ç»ƒæ¨¡å‹åŠ è½½"""
        try:
            logger.info(f"ğŸ“¥ å°è¯•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {self.model_name}")
            
            # é¦–å…ˆå°è¯•åŠ è½½é…ç½®
            config = AutoConfig.from_pretrained(
                self.model_name,
                num_labels=self.num_labels,
                problem_type="multi_label_classification",
                cache_dir=MODEL_PATHS["pretrained_cache"]
            )
            
            # å°è¯•åŠ è½½åˆ†è¯å™¨ - å¤šç§ç­–ç•¥
            self.tokenizer = self._load_tokenizer_safe()
            
            # åŠ è½½æ¨¡å‹
            self.model = AutoModelForSequenceClassification.from_pretrained(
                self.model_name,
                config=config,
                cache_dir=MODEL_PATHS["pretrained_cache"]
            )
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            self.model.to(self.device)
            self.model_loaded = True
            
            logger.info("âœ… é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ é¢„è®­ç»ƒæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œä¿æŒä¼˜é›…é™çº§
    
    def _load_tokenizer_safe(self):
        """å®‰å…¨çš„åˆ†è¯å™¨åŠ è½½"""
        tokenizer = None
        
        # ç­–ç•¥1: ç›´æ¥ä»æ¨¡å‹è·¯å¾„åŠ è½½
        try:
            tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.tokenizer_loaded = True
            logger.info("âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ (ç­–ç•¥1)")
            return tokenizer
        except Exception as e:
            logger.warning(f"åˆ†è¯å™¨åŠ è½½ç­–ç•¥1å¤±è´¥: {e}")
        
        # ç­–ç•¥2: ä»åŸºç¡€æ¨¡å‹åŠ è½½
        try:
            tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base")
            self.tokenizer_loaded = True
            logger.info("âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ (ç­–ç•¥2: åŸºç¡€æ¨¡å‹)")
            return tokenizer
        except Exception as e:
            logger.error(f"æ‰€æœ‰åˆ†è¯å™¨åŠ è½½ç­–ç•¥å¤±è´¥: {e}")
            
        return tokenizer
    
    def load_finetuned_model_safe(self, model_path: str = None):
        """å®‰å…¨çš„å¾®è°ƒæ¨¡å‹åŠ è½½"""
        try:
            model_path = model_path or MODEL_PATHS["finetuned_model"]
            logger.info(f"ğŸ“¥ å°è¯•åŠ è½½å¾®è°ƒæ¨¡å‹: {model_path}")
            
            # æ£€æŸ¥è·¯å¾„å­˜åœ¨æ€§
            from pathlib import Path
            model_path = Path(model_path)
            if not model_path.exists():
                raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            
            # ç­–ç•¥1: ç›´æ¥åŠ è½½
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(str(model_path))
                self.model = AutoModelForSequenceClassification.from_pretrained(str(model_path))
                self.model.to(self.device)
                self.model.eval()
                self.model_loaded = True
                self.tokenizer_loaded = True
                logger.info("âœ… å¾®è°ƒæ¨¡å‹åŠ è½½æˆåŠŸ (ç­–ç•¥1)")
                return True
                
            except Exception as e1:
                logger.warning(f"å¾®è°ƒæ¨¡å‹åŠ è½½ç­–ç•¥1å¤±è´¥: {e1}")
                
                # ç­–ç•¥2: åˆ†åˆ«å¤„ç†åˆ†è¯å™¨å’Œæ¨¡å‹
                try:
                    # ä½¿ç”¨åŸºç¡€åˆ†è¯å™¨
                    self.tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base")
                    self.tokenizer_loaded = True
                    
                    # åŠ è½½å¾®è°ƒçš„æ¨¡å‹æƒé‡
                    self.model = AutoModelForSequenceClassification.from_pretrained(str(model_path))
                    self.model.to(self.device)
                    self.model.eval()
                    self.model_loaded = True
                    
                    logger.info("âœ… å¾®è°ƒæ¨¡å‹åŠ è½½æˆåŠŸ (ç­–ç•¥2)")
                    return True
                    
                except Exception as e2:
                    logger.error(f"å¾®è°ƒæ¨¡å‹åŠ è½½ç­–ç•¥2å¤±è´¥: {e2}")
                    raise e2
            
        except Exception as e:
            logger.error(f"âŒ å¾®è°ƒæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # å›é€€åˆ°é¢„è®­ç»ƒæ¨¡å‹
            logger.info("ğŸ”„ å›é€€åˆ°é¢„è®­ç»ƒæ¨¡å‹")
            self._load_pretrained_model_safe()
            return False
    
    def predict_single(self, text: str, return_dict: bool = False) -> Union[np.ndarray, Dict[str, float]]:
        """å…¼å®¹ç‰ˆå•æ–‡æœ¬é¢„æµ‹"""
        try:
            # æ£€æŸ¥æ¨¡å‹çŠ¶æ€
            if not self.model_loaded or not self.tokenizer_loaded:
                logger.warning("âš ï¸ æ¨¡å‹æœªæ­£ç¡®åŠ è½½ï¼Œè¿”å›é›¶å‘é‡")
                zero_vector = np.zeros(27, dtype=np.float32)
                return self.mapper.map_ck_vector_to_dict(zero_vector) if return_dict else zero_vector
            
            if not text or len(text.strip()) < 1:
                logger.warning("âš ï¸ è¾“å…¥æ–‡æœ¬ä¸ºç©ºï¼Œè¿”å›é›¶å‘é‡")
                zero_vector = np.zeros(27, dtype=np.float32)
                return self.mapper.map_ck_vector_to_dict(zero_vector) if return_dict else zero_vector
            
            # æ–‡æœ¬é¢„å¤„ç†å’Œåˆ†è¯
            inputs = self.tokenizer(
                text,
                padding=True,
                truncation=True,
                max_length=MODEL_CONFIG["max_length"],
                return_tensors="pt"
            )
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # æ¨¡å‹æ¨ç†
            self.model.eval()
            with torch.no_grad():
                outputs = self.model(**inputs)
                logits = outputs.logits
                
                # åº”ç”¨sigmoidæ¿€æ´» (å¤šæ ‡ç­¾åˆ†ç±»)
                probabilities = torch.sigmoid(logits).cpu().numpy().flatten()
            
            # ç¡®ä¿è¾“å‡ºç»´åº¦æ­£ç¡®
            if len(probabilities) != 27:
                logger.error(f"âŒ æ¨¡å‹è¾“å‡ºç»´åº¦é”™è¯¯: æœŸæœ›27ç»´ï¼Œå®é™…{len(probabilities)}ç»´")
                probabilities = np.zeros(27, dtype=np.float32)
            
            # åº”ç”¨ç½®ä¿¡åº¦é˜ˆå€¼
            threshold = INFERENCE_CONFIG["confidence_threshold"]
            probabilities = np.where(probabilities > threshold, probabilities, 0.0)
            
            # å½’ä¸€åŒ–åˆ°[0, 1]
            probabilities = np.clip(probabilities, 0, 1)
            
            if return_dict:
                return self.mapper.map_ck_vector_to_dict(probabilities)
            else:
                return probabilities.astype(np.float32)
                
        except Exception as e:
            logger.error(f"âŒ å•æ–‡æœ¬é¢„æµ‹å¤±è´¥: {e}")
            zero_vector = np.zeros(27, dtype=np.float32)
            return self.mapper.map_ck_vector_to_dict(zero_vector) if return_dict else zero_vector
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'model_loaded': self.model_loaded,
            'tokenizer_loaded': self.tokenizer_loaded,
            'device': self.device,
            'model_name': self.model_name,
            'num_labels': self.num_labels
        }

# å…¼å®¹æ€§åŒ…è£…å‡½æ•°
def create_compatible_classifier(**kwargs):
    """åˆ›å»ºå…¼å®¹ç‰ˆåˆ†ç±»å™¨çš„å·¥å‚å‡½æ•°"""
    return CompatibleEmotionClassifier(**kwargs)

if __name__ == "__main__":
    # æµ‹è¯•å…¼å®¹ç‰ˆåˆ†ç±»å™¨
    print("ğŸ§ª æµ‹è¯•å…¼å®¹ç‰ˆæƒ…æ„Ÿåˆ†ç±»å™¨")
    classifier = CompatibleEmotionClassifier(load_pretrained=True)
    
    # å°è¯•åŠ è½½å¾®è°ƒæ¨¡å‹
    try:
        classifier.load_finetuned_model_safe()
    except:
        pass
    
    # æ˜¾ç¤ºçŠ¶æ€
    info = classifier.get_model_info()
    print(f"æ¨¡å‹çŠ¶æ€: {info}")
    
    # æµ‹è¯•é¢„æµ‹
    test_text = "æˆ‘ä»Šå¤©å¾ˆå¼€å¿ƒ"
    result = classifier.predict_single(test_text)
    print(f"æµ‹è¯•ç»“æœ: {result[:5]}... (å‰5ç»´)")
'''
            
            # ä¿å­˜å…¼å®¹ç‰ˆåˆ†ç±»å™¨
            compatible_file = self.ac_root / "debug" / "emotion_classifier_compatible.py"
            with open(compatible_file, 'w', encoding='utf-8') as f:
                f.write(compatible_code)
            
            logger.info(f"âœ… å…¼å®¹ç‰ˆåˆ†ç±»å™¨å·²ä¿å­˜: {compatible_file}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºå…¼å®¹åˆ†ç±»å™¨å¤±è´¥: {e}")
            return False
    
    def create_quick_fix_script(self) -> bool:
        """åˆ›å»ºå¿«é€Ÿä¿®å¤è„šæœ¬"""
        logger.info("âš¡ åˆ›å»ºå¿«é€Ÿä¿®å¤è„šæœ¬...")
        
        fix_script = '''#!/usr/bin/env python3
"""
ACæ¨¡å—å¿«é€Ÿä¿®å¤è„šæœ¬

ä½¿ç”¨æ–¹æ³•:
1. python debug/quick_fix.py --backup     # å¤‡ä»½åŸå§‹æ–‡ä»¶
2. python debug/quick_fix.py --fix        # åº”ç”¨ä¿®å¤
3. python debug/quick_fix.py --test       # æµ‹è¯•ä¿®å¤ç»“æœ
4. python debug/quick_fix.py --restore    # æ¢å¤åŸå§‹æ–‡ä»¶
"""

import argparse
import sys
import shutil
from pathlib import Path

def backup_files():
    """å¤‡ä»½åŸå§‹æ–‡ä»¶"""
    ac_root = Path(__file__).parent.parent
    model_dir = ac_root / "models" / "finetuned_xlm_roberta"
    backup_dir = ac_root / "debug" / "backup"
    backup_dir.mkdir(exist_ok=True)
    
    files_to_backup = [
        "emotion_classifier.py",
        "inference_api.py",
    ]
    
    model_files_to_backup = [
        "tokenizer.json",
        "tokenizer_config.json"
    ]
    
    print("ğŸ“¦ å¤‡ä»½åŸå§‹æ–‡ä»¶...")
    
    # å¤‡ä»½ACæ¨¡å—æ–‡ä»¶
    for file in files_to_backup:
        src = ac_root / file
        dst = backup_dir / file
        if src.exists():
            shutil.copy2(src, dst)
            print(f"âœ… å·²å¤‡ä»½: {file}")
    
    # å¤‡ä»½æ¨¡å‹æ–‡ä»¶
    for file in model_files_to_backup:
        src = model_dir / file
        dst = backup_dir / file
        if src.exists():
            shutil.copy2(src, dst)
            print(f"âœ… å·²å¤‡ä»½: {file}")
    
    print("âœ… å¤‡ä»½å®Œæˆ")

def apply_fix():
    """åº”ç”¨ä¿®å¤"""
    ac_root = Path(__file__).parent.parent
    
    print("ğŸ”§ åº”ç”¨ç‰ˆæœ¬å…¼å®¹ä¿®å¤...")
    
    # 1. é‡æ–°ç”Ÿæˆå…¼å®¹çš„åˆ†è¯å™¨
    try:
        from transformers import AutoTokenizer
        model_dir = ac_root / "models" / "finetuned_xlm_roberta"
        
        # ä½¿ç”¨åŸºç¡€æ¨¡å‹åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base")
        tokenizer.save_pretrained(str(model_dir))
        print("âœ… åˆ†è¯å™¨å·²ä¿®å¤")
    except Exception as e:
        print(f"âŒ åˆ†è¯å™¨ä¿®å¤å¤±è´¥: {e}")
    
    # 2. æ›¿æ¢ä¸ºå…¼å®¹ç‰ˆemotion_classifier
    try:
        compatible_file = ac_root / "debug" / "emotion_classifier_compatible.py"
        target_file = ac_root / "emotion_classifier.py"
        
        if compatible_file.exists():
            shutil.copy2(compatible_file, target_file)
            print("âœ… æƒ…æ„Ÿåˆ†ç±»å™¨å·²æ›¿æ¢ä¸ºå…¼å®¹ç‰ˆ")
    except Exception as e:
        print(f"âŒ åˆ†ç±»å™¨æ›¿æ¢å¤±è´¥: {e}")
    
    print("âœ… ä¿®å¤åº”ç”¨å®Œæˆ")

def test_fix():
    """æµ‹è¯•ä¿®å¤ç»“æœ"""
    print("ğŸ§ª æµ‹è¯•ä¿®å¤ç»“æœ...")
    
    try:
        # å¯¼å…¥å¹¶æµ‹è¯•å…¼å®¹ç‰ˆåˆ†ç±»å™¨
        sys.path.insert(0, str(Path(__file__).parent.parent))
        from emotion_classifier import CompatibleEmotionClassifier
        
        # åˆ›å»ºåˆ†ç±»å™¨å®ä¾‹
        classifier = CompatibleEmotionClassifier(load_pretrained=False)
        
        # å°è¯•åŠ è½½å¾®è°ƒæ¨¡å‹
        result = classifier.load_finetuned_model_safe()
        
        # è·å–æ¨¡å‹çŠ¶æ€
        info = classifier.get_model_info()
        print(f"æ¨¡å‹çŠ¶æ€: {info}")
        
        # æµ‹è¯•é¢„æµ‹åŠŸèƒ½
        if info['model_loaded'] and info['tokenizer_loaded']:
            test_texts = [
                "æˆ‘ä»Šå¤©å¾ˆå¼€å¿ƒ",
                "I feel sad",
                "This is frustrating"
            ]
            
            print("\\nğŸ§ª æµ‹è¯•é¢„æµ‹:")
            for text in test_texts:
                result = classifier.predict_single(text)
                active_emotions = sum(1 for x in result if x > 0.1)
                print(f"æ–‡æœ¬: '{text}' -> æ´»è·ƒæƒ…ç»ªæ•°: {active_emotions}, å‘é‡å’Œ: {sum(result):.3f}")
            
            print("âœ… åŠŸèƒ½æµ‹è¯•é€šè¿‡")
        else:
            print("âŒ æ¨¡å‹æœªæ­£ç¡®åŠ è½½")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")

def restore_files():
    """æ¢å¤åŸå§‹æ–‡ä»¶"""
    print("ğŸ”„ æ¢å¤åŸå§‹æ–‡ä»¶...")
    
    ac_root = Path(__file__).parent.parent
    backup_dir = ac_root / "debug" / "backup"
    
    if not backup_dir.exists():
        print("âŒ æœªæ‰¾åˆ°å¤‡ä»½ç›®å½•")
        return
    
    files_to_restore = [
        "emotion_classifier.py",
        "inference_api.py"
    ]
    
    for file in files_to_restore:
        src = backup_dir / file
        dst = ac_root / file
        if src.exists():
            shutil.copy2(src, dst)
            print(f"âœ… å·²æ¢å¤: {file}")
    
    print("âœ… æ¢å¤å®Œæˆ")

def main():
    parser = argparse.ArgumentParser(description="ACæ¨¡å—å¿«é€Ÿä¿®å¤å·¥å…·")
    parser.add_argument("--backup", action="store_true", help="å¤‡ä»½åŸå§‹æ–‡ä»¶")
    parser.add_argument("--fix", action="store_true", help="åº”ç”¨ä¿®å¤")
    parser.add_argument("--test", action="store_true", help="æµ‹è¯•ä¿®å¤ç»“æœ")
    parser.add_argument("--restore", action="store_true", help="æ¢å¤åŸå§‹æ–‡ä»¶")
    parser.add_argument("--all", action="store_true", help="æ‰§è¡Œå®Œæ•´ä¿®å¤æµç¨‹")
    
    args = parser.parse_args()
    
    if args.all:
        backup_files()
        apply_fix()
        test_fix()
    elif args.backup:
        backup_files()
    elif args.fix:
        apply_fix()
    elif args.test:
        test_fix()
    elif args.restore:
        restore_files()
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
'''
        
        # ä¿å­˜ä¿®å¤è„šæœ¬
        fix_script_path = self.ac_root / "debug" / "quick_fix.py"
        with open(fix_script_path, 'w', encoding='utf-8') as f:
            f.write(fix_script)
        
        # æ·»åŠ æ‰§è¡Œæƒé™
        import stat
        fix_script_path.chmod(fix_script_path.stat().st_mode | stat.S_IEXEC)
        
        logger.info(f"âœ… å¿«é€Ÿä¿®å¤è„šæœ¬å·²åˆ›å»º: {fix_script_path}")
        return True
    
    def run_complete_fix(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´ä¿®å¤æµç¨‹"""
        logger.info("ğŸ”§ å¼€å§‹å®Œæ•´ä¿®å¤æµç¨‹...")
        
        results = {
            'diagnosis': self.diagnose_version_issues(),
            'fixes_applied': [],
            'success': True
        }
        
        # 1. ä¿®å¤åˆ†è¯å™¨å…¼å®¹æ€§
        if self.fix_tokenizer_compatibility():
            results['fixes_applied'].append('tokenizer_compatibility')
        
        # 2. åˆ›å»ºå…¼å®¹çš„åˆ†ç±»å™¨
        if self.create_compatible_emotion_classifier():
            results['fixes_applied'].append('compatible_classifier')
        
        # 3. åˆ›å»ºå¿«é€Ÿä¿®å¤è„šæœ¬
        if self.create_quick_fix_script():
            results['fixes_applied'].append('quick_fix_script')
        
        # 4. ä¿å­˜ä¿®å¤æŠ¥å‘Š
        report_path = self.ac_root / "debug" / "compatibility_fix_report.json"
        import json
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“„ ä¿®å¤æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        logger.info(f"âœ… ä¿®å¤æµç¨‹å®Œæˆï¼Œåº”ç”¨äº† {len(results['fixes_applied'])} ä¸ªä¿®å¤")
        
        return results

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ ACæ¨¡å—ç‰ˆæœ¬å…¼å®¹æ€§ä¿®å¤å·¥å…·")
    print("="*60)
    
    fixer = VersionCompatibilityFixer()
    
    # è¿è¡Œå®Œæ•´ä¿®å¤
    results = fixer.run_complete_fix()
    
    print("\\n" + "="*60)
    print("ğŸ“‹ ä¿®å¤æ‘˜è¦:")
    print(f"   åº”ç”¨ä¿®å¤æ•°: {len(results['fixes_applied'])}")
    print(f"   ä¿®å¤é¡¹ç›®: {results['fixes_applied']}")
    print("\\nğŸ’¡ ä¸‹ä¸€æ­¥æ“ä½œ:")
    print("1. è¿è¡Œ: python debug/quick_fix.py --test")
    print("2. æˆ–ç›´æ¥ä½¿ç”¨å…¼å®¹ç‰ˆåˆ†ç±»å™¨æµ‹è¯•åŠŸèƒ½")
    
    return results

if __name__ == "__main__":
    main()