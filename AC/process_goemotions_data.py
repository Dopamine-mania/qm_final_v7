#!/usr/bin/env python3
"""
å¤„ç†GoEmotionsæ•°æ®å¹¶è½¬æ¢ä¸ºC&Kæ ¼å¼

ä¿®å¤å¯¼å…¥é—®é¢˜å¹¶å®Œæˆæ•°æ®è½¬æ¢
"""

import sys
import os
import pandas as pd
import numpy as np
import logging
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from emotion_mapper import GoEmotionsMapper
from config import COWEN_KELTNER_EMOTIONS, GOEMOTIONS_LABELS

logger = logging.getLogger(__name__)

def process_goemotions_to_ck():
    """å¤„ç†GoEmotionsæ•°æ®å¹¶è½¬æ¢ä¸ºC&Kæ ¼å¼"""
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    data_dir = Path(__file__).parent / "data"
    mapper = GoEmotionsMapper()
    
    splits = ['train', 'dev', 'test']
    
    for split in splits:
        logger.info(f"\nğŸ”„ å¤„ç† {split} æ•°æ®é›†...")
        
        # è¯»å–GoEmotionsæ ¼å¼æ•°æ®
        ge_file = data_dir / f"goemotions_{split}.csv"
        if not ge_file.exists():
            logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {ge_file}")
            continue
        
        logger.info(f"ğŸ“‚ è¯»å–æ–‡ä»¶: {ge_file}")
        df = pd.read_csv(ge_file)
        logger.info(f"   æ•°æ®é‡: {len(df)} æ¡è®°å½•")
        
        # è½¬æ¢ä¸ºC&Kæ ¼å¼
        logger.info("ğŸ”„ è½¬æ¢ä¸ºC&K 27ç»´æ ¼å¼...")
        
        result_data = []
        conversion_stats = {ck_emotion: 0 for ck_emotion in COWEN_KELTNER_EMOTIONS}
        
        for idx, row in df.iterrows():
            try:
                text = row['text']
                
                # æå–GoEmotionsåˆ†æ•°
                ge_scores = {}
                for label in GOEMOTIONS_LABELS:
                    if label in df.columns:
                        ge_scores[label] = float(row[label]) if pd.notna(row[label]) else 0.0
                
                # æ˜ å°„åˆ°C&Kå‘é‡
                ck_vector = mapper.map_goemotions_to_ck_vector(ge_scores)
                
                # æ„å»ºæ•°æ®è¡Œ
                data_row = {'text': text}
                
                # æ·»åŠ C&Kæƒ…ç»ªåˆ—
                for i, emotion in enumerate(COWEN_KELTNER_EMOTIONS):
                    data_row[emotion] = float(ck_vector[i])
                    if ck_vector[i] > 0:
                        conversion_stats[emotion] += 1
                
                # æ·»åŠ å…ƒæ•°æ®
                active_ge_labels = [label for label, score in ge_scores.items() if score > 0]
                data_row['original_goemotions'] = ','.join(active_ge_labels)
                data_row['max_emotion'] = COWEN_KELTNER_EMOTIONS[np.argmax(ck_vector)]
                data_row['emotion_intensity'] = float(np.max(ck_vector))
                data_row['total_intensity'] = float(np.sum(ck_vector))
                
                result_data.append(data_row)
                
                if (idx + 1) % 5000 == 0:
                    logger.info(f"   è½¬æ¢è¿›åº¦: {idx + 1}/{len(df)}")
                    
            except Exception as e:
                logger.warning(f"âš ï¸  ç¬¬{idx}è¡Œè½¬æ¢å¤±è´¥: {e}")
                continue
        
        # ä¿å­˜ç»“æœ
        result_df = pd.DataFrame(result_data)
        output_path = data_dir / f"processed_{split}.csv"
        result_df.to_csv(output_path, index=False, encoding='utf-8')
        
        logger.info(f"âœ… C&Kæ ¼å¼ä¿å­˜: {output_path}")
        logger.info(f"   è½¬æ¢æˆåŠŸ: {len(result_df)} æ¡è®°å½•")
        
        # ç»Ÿè®¡C&Kæƒ…ç»ªåˆ†å¸ƒ
        logger.info("ğŸ“Š C&Kæƒ…ç»ªåˆ†å¸ƒç»Ÿè®¡:")
        sorted_ck = sorted(conversion_stats.items(), key=lambda x: x[1], reverse=True)
        for emotion, count in sorted_ck[:10]:
            logger.info(f"   {emotion}: {count}")
    
    logger.info("\nâœ… æ‰€æœ‰æ•°æ®è½¬æ¢å®Œæˆ!")

def verify_processed_data():
    """éªŒè¯å¤„ç†åçš„æ•°æ®"""
    logger.info("\nğŸ” éªŒè¯å¤„ç†åçš„æ•°æ®...")
    
    data_dir = Path(__file__).parent / "data"
    splits = ['train', 'dev', 'test']
    
    summary = {}
    
    for split in splits:
        ck_file = data_dir / f"processed_{split}.csv"
        if ck_file.exists():
            df = pd.read_csv(ck_file)
            
            # åŸºç¡€ç»Ÿè®¡
            emotion_columns = [col for col in df.columns if col in COWEN_KELTNER_EMOTIONS]
            emotion_matrix = df[emotion_columns].values
            
            stats = {
                "samples": len(df),
                "emotion_dimensions": len(emotion_columns),
                "avg_emotions_per_sample": float(np.mean(np.sum(emotion_matrix > 0, axis=1))),
                "avg_total_intensity": float(np.mean(np.sum(emotion_matrix, axis=1))),
                "most_common_emotion": emotion_columns[np.argmax(np.sum(emotion_matrix, axis=0))] if len(emotion_columns) > 0 else "N/A"
            }
            
            summary[split] = stats
            
            logger.info(f"ğŸ“Š {split} æ•°æ®é›†:")
            logger.info(f"   æ ·æœ¬æ•°: {stats['samples']}")
            logger.info(f"   æƒ…ç»ªç»´åº¦: {stats['emotion_dimensions']}")
            logger.info(f"   å¹³å‡æ´»è·ƒæƒ…ç»ª: {stats['avg_emotions_per_sample']:.2f}")
            logger.info(f"   å¹³å‡æ€»å¼ºåº¦: {stats['avg_total_intensity']:.3f}")
            logger.info(f"   æœ€å¸¸è§æƒ…ç»ª: {stats['most_common_emotion']}")
            
            # æ£€æŸ¥æ•°æ®è´¨é‡
            text_lengths = df['text'].str.len()
            logger.info(f"   æ–‡æœ¬é•¿åº¦: å¹³å‡{text_lengths.mean():.1f}, æœ€å¤§{text_lengths.max()}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ— æ•ˆæ•°æ®
            invalid_rows = df[df[emotion_columns].sum(axis=1) == 0]
            if len(invalid_rows) > 0:
                logger.warning(f"âš ï¸  å‘ç° {len(invalid_rows)} æ¡æ— æƒ…ç»ªæ ‡ç­¾çš„æ•°æ®")
    
    # ä¿å­˜éªŒè¯æŠ¥å‘Š
    import json
    report_path = data_dir / "data_validation_report.json"
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    logger.info(f"\nâœ… éªŒè¯æŠ¥å‘Šä¿å­˜: {report_path}")
    return summary

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”„ å¤„ç†GoEmotionsæ•°æ®å¹¶è½¬æ¢ä¸ºC&Kæ ¼å¼")
    print("=" * 50)
    
    try:
        # å¤„ç†æ•°æ®
        process_goemotions_to_ck()
        
        # éªŒè¯æ•°æ®
        summary = verify_processed_data()
        
        # æ˜¾ç¤ºæ€»ç»“
        print("\nğŸ“‹ æ•°æ®å¤„ç†æ€»ç»“:")
        total_samples = sum(info['samples'] for info in summary.values())
        print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"æ•°æ®åˆ†å‰²: {list(summary.keys())}")
        
        print("\nğŸ‰ æ•°æ®å¤„ç†å®Œæˆï¼ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒæ¨¡å‹äº†ã€‚")
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®å¤„ç†å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    success = main()
    if not success:
        exit(1)